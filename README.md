# Tutoriel DSPy et GEPA : Classification automatique de billets d'assistance informatique

Ce tutoriel explique **DSPy** (Declarative Self-improving Language Programs) et **GEPA** (Genetic-Pareto Algorithm), deux outils pour d√©velopper des applications utilisant des mod√®les de langage.

## Vue d'ensemble

DSPy est un cadre de d√©veloppement qui permet de cr√©er des programmes utilisant des mod√®les de langage de mani√®re d√©clarative et maintenable. Plut√¥t que d'√©crire des instructions manuellement, vous d√©finissez ce que vous voulez accomplir, et DSPy optimise automatiquement les instructions et les exemples.

GEPA est l'optimiseur le plus sophistiqu√© de DSPy. Il utilise des algorithmes g√©n√©tiques et la r√©flexion par mod√®le de langage pour trouver automatiquement les meilleures instructions possibles.

## Objectifs d'apprentissage

√Ä la fin de ce tutoriel, vous serez en mesure de :

1. D√©finir des **signatures** DSPy pour d√©clarer vos t√¢ches
2. Cr√©er et composer des **modules** DSPy
3. √âvaluer la performance avec des **m√©triques personnalis√©es**
4. Optimiser automatiquement avec les **optimiseurs** (BootstrapFewShot, MIPRO, GEPA)
5. Utiliser diff√©rents **mod√®les de langage** de mani√®re interchangeable
6. Impl√©menter des **motifs avanc√©s** pour la production

## Pr√©requis

### Logiciels requis

- **Python 3.9+**
- **Ollama** (pour ex√©cuter des mod√®les localement)
- **uv** (gestionnaire de paquets Python recommand√©)

### Installation

1. **Installer Ollama**

   Visitez [ollama.ai](https://ollama.ai) et suivez les instructions pour votre syst√®me d'exploitation.

2. **T√©l√©charger un mod√®le**

   ```bash
   ollama pull llama3.1:8b
   ```

3. **Cloner ce d√©p√¥t**

   ```bash
   git clone https://github.com/votre-utilisateur/dspy_gepa_demo.git
   cd dspy_gepa_demo
   ```

4. **Installer les d√©pendances**

   Avec uv (recommand√©) :
   ```bash
   uv pip install dspy-ai
   ```

   Ou avec pip :
   ```bash
   pip install dspy-ai
   ```

## Structure du projet

```
dspy_gepa_demo/
‚îú‚îÄ‚îÄ README.md                 # Ce fichier
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration de DSPy et des mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ data.py              # Donn√©es d'entra√Ænement et de validation
‚îÇ   ‚îú‚îÄ‚îÄ signatures.py        # D√©finitions des signatures DSPy
‚îÇ   ‚îú‚îÄ‚îÄ modules.py           # Modules DSPy (Predict, ChainOfThought, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # M√©triques d'√©valuation
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py        # Fonctions d'√©valuation
‚îÇ   ‚îú‚îÄ‚îÄ optimizers.py        # Optimiseurs (BootstrapFewShot, MIPRO)
‚îÇ   ‚îú‚îÄ‚îÄ gepa_utils.py        # Utilitaires GEPA
‚îÇ   ‚îú‚îÄ‚îÄ patterns.py          # Motifs avanc√©s (validation, retry, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ multi_model.py       # Configuration multi-mod√®les
‚îÇ   ‚îî‚îÄ‚îÄ examples.py          # Exemples d'utilisation
‚îî‚îÄ‚îÄ images/                   # Images pour la documentation
```

## Concepts fondamentaux

### 1. Les signatures DSPy

#### Qu'est-ce qu'une signature ?

Une signature dans DSPy est une d√©claration de ce que votre programme doit faire, pas comment le faire.

#### Analogie

Imaginez que vous engagez un assistant :

- ‚ùå Sans DSPy : Vous lui donnez des instructions d√©taill√©es ("d'abord lis le ticket, ensuite regarde si c'est hardware...")
- ‚úÖ Avec DSPy : Vous lui donnez simplement le contrat ("je te donne un ticket, tu me donnes la cat√©gorie et la priorit√©")

#### Composants d'une signature

- **Docstring** : Description de la t√¢che
- **Champs d'entr√©e** (`InputField`) : Ce que vous fournissez
- **Champs de sortie** (`OutputField`) : Ce que vous attendez
- **Descriptions** (optionnelles) : Pr√©cisions sur chaque champ

#### Exemple 1 : Signature simple

La forme la plus basique d'une signature.

```python
import dspy

class BasicSignature(dspy.Signature):
    """Classifier un ticket IT."""
    
    ticket = dspy.InputField()
    category = dspy.OutputField()
```

#### Exemple 2 : Signature avec descriptions

Ajouter des descriptions aide le mod√®le √† mieux comprendre la t√¢che.

```python
class DescriptiveSignature(dspy.Signature):
    """Classifier un ticket de support IT selon sa cat√©gorie."""
    
    ticket = dspy.InputField(desc="Description du probl√®me rapport√© par l'utilisateur")
    category = dspy.OutputField(desc="Cat√©gorie technique du probl√®me")
```

#### Exemple 3 : Signature avec contraintes

Sp√©cifier les valeurs possibles dans la description.

```python
import dspy

class TicketClassifier(dspy.Signature):
    """Classer un billet d'assistance informatique selon sa cat√©gorie et sa priorit√©."""

    ticket = dspy.InputField(desc="Description du billet d'assistance informatique")
    category = dspy.OutputField(desc="Cat√©gorie parmi: Hardware, Software, Network, etc.")
    priority = dspy.OutputField(desc="Priorit√© parmi: Low, Medium, High, Urgent, Critical")
```

#### ‚úÖ Bonnes Pratiques pour les signatures

- **Docstring claire** : D√©crivez la t√¢che en une phrase
- **Noms explicites** : ticket plut√¥t que input, category plut√¥t que output
- **Descriptions pr√©cises** : Ajoutez desc pour guider le mod√®le
- **Contraintes claires** : Listez les valeurs possibles quand applicable
- **Commencer simple** : Ajoutez des champs progressivement


### 2. Modules : Ex√©cuter vos t√¢ches

## Qu'est-ce qu'un module ?

Un **module** dans DSPy est un composant qui **utilise une signature** pour g√©n√©rer des pr√©dictions.

**Analogie :**
- **Signature** = Le contrat ("je te donne X, tu me donnes Y")
- **Module** = L'employ√© qui ex√©cute le contrat (avec sa propre m√©thode de travail)

DSPy offre plusieurs types de modules, chacun avec une strat√©gie diff√©rente :

#### Module 1 : Predict (le plus simple)

**Predict** est le module de base : il g√©n√®re directement une r√©ponse.

**Fonctionnement :**
1. Re√ßoit les entr√©es
2. G√©n√®re imm√©diatement les sorties
3. Retourne le r√©sultat

**Quand l'utiliser :**
- T√¢ches simples
- Besoin de rapidit√©
- Premi√®re version d'un syst√®me

```python
import dspy

class SimpleClassifier(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.Predict(TicketClassifier)

    def forward(self, ticket):
        return self.predictor(ticket=ticket)
```

#### Module 2 : ChainOfThought (avec raisonnement)

**ChainOfThought** demande au mod√®le de raisonner avant de r√©pondre.

**Fonctionnement :**
1. Re√ßoit les entr√©es
2. **G√©n√®re d'abord un raisonnement**
3. G√©n√®re ensuite les sorties bas√©es sur ce raisonnement
4. Retourne le r√©sultat (avec le raisonnement)

**Quand l'utiliser :**
- T√¢ches complexes n√©cessitant de la r√©flexion
- Besoin d'expliquer les d√©cisions
- Am√©liorer la pr√©cision (+5-15% typiquement)

**Avantages :**
- ‚úÖ Meilleure pr√©cision
- ‚úÖ Raisonnement explicite et auditable
- ‚úÖ Meilleure gestion des cas limites

**Inconv√©nients :**
- ‚ùå Plus lent (g√©n√®re plus de tokens)
- ‚ùå Plus co√ªteux en appels LLM

```python
class ThinkingClassifier(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(TicketClassifier)

    def forward(self, ticket):
        return self.predictor(ticket=ticket)
```

**Exemple de sortie avec raisonnement :**
```
Raisonnement: "L'utilisateur mentionne que son ordinateur ne d√©marre plus.
Il s'agit clairement d'un probl√®me mat√©riel. De plus, il a une pr√©sentation
dans 1 heure, ce qui rend le probl√®me urgent."

Cat√©gorie: Hardware
Priorit√©: Urgent
```

#### Module 3 : ReAct (raisonnement + actions)

**ReAct** alterne entre raisonnement et actions.

**Fonctionnement :**
1. Raisonne sur le probl√®me
2. D√©cide d'une action √† faire (ex: chercher dans une base de donn√©es)
3. Observe le r√©sultat de l'action
4. Raisonne √† nouveau avec cette nouvelle information
5. R√©p√®te jusqu'√† avoir la r√©ponse

**Quand l'utiliser :**
- Besoin d'interactions avec des outils externes
- Recherche d'informations n√©cessaire
- T√¢ches multi-√©tapes

**Note :** ReAct n√©cessite de d√©finir des outils (fonctions) que le mod√®le peut appeler.

#### Module 4 : ProgramOfThought (g√©n√©ration de code)

**ProgramOfThought** g√©n√®re du code Python pour raisonner.

**Fonctionnement :**
1. Analyse le probl√®me
2. G√©n√®re du code Python pour le r√©soudre
3. Ex√©cute le code
4. Utilise le r√©sultat pour g√©n√©rer la r√©ponse

**Quand l'utiliser :**
- Probl√®mes math√©matiques
- Calculs complexes
- Manipulation de donn√©es structur√©es

**Exemple typique :** "Combien font 347 * 892 + 123 / 7 ?"
- Le mod√®le g√©n√®re : `result = 347 * 892 + 123 / 7`
- Ex√©cute le code : `309541.57`
- Retourne la r√©ponse avec le calcul exact

#### Module 5 : Modules personnalis√©s (composition)

Vous pouvez cr√©er vos propres modules en **composant** plusieurs modules existants.

**Pourquoi composer des modules ?**
- D√©composer une t√¢che complexe en sous-t√¢ches
- R√©utiliser des modules existants
- Cr√©er des pipelines sophistiqu√©s

**Exemple 1 : Pipeline s√©quentiel**

Classifier d'abord la cat√©gorie, puis la priorit√© en fonction de la cat√©gorie.

```python
class SequentialClassifier(dspy.Module):
    def __init__(self):
        super().__init__()
        self.category_predictor = dspy.ChainOfThought(CategoryClassifier)
        self.priority_predictor = dspy.ChainOfThought(PriorityClassifier)

    def forward(self, ticket):
        # √âtape 1 : Pr√©dire la cat√©gorie
        category_result = self.category_predictor(ticket=ticket)

        # √âtape 2 : Pr√©dire la priorit√© en utilisant la cat√©gorie
        priority_result = self.priority_predictor(
            ticket=ticket,
            category=category_result.category
        )

        return dspy.Prediction(
            category=category_result.category,
            priority=priority_result.priority
        )
```

**Exemple 2 : Module avec validation**

Ajouter une √©tape de validation pour v√©rifier que les pr√©dictions sont valides.

```python
class ValidatedClassifier(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(TicketClassifier)
        self.valid_categories = {"Hardware", "Software", "Network", "Access"}
        self.valid_priorities = {"Low", "Medium", "High", "Urgent", "Critical"}

    def forward(self, ticket):
        result = self.predictor(ticket=ticket)

        # Validation et normalisation
        if result.category not in self.valid_categories:
            result.category = "Software"  # Cat√©gorie par d√©faut
        if result.priority not in self.valid_priorities:
            result.priority = "Medium"  # Priorit√© par d√©faut

        return result
```

**Exemple 3 : Module avec consensus (ensemble)**

Utiliser plusieurs modules et combiner leurs pr√©dictions.

```python
class EnsembleClassifier(dspy.Module):
    def __init__(self):
        super().__init__()
        self.classifiers = [
            dspy.Predict(TicketClassifier),
            dspy.ChainOfThought(TicketClassifier),
            dspy.ChainOfThought(TicketClassifier)  # 2x ChainOfThought
        ]

    def forward(self, ticket):
        predictions = [clf(ticket=ticket) for clf in self.classifiers]

        # Vote majoritaire pour la cat√©gorie
        categories = [p.category for p in predictions]
        category = max(set(categories), key=categories.count)

        # Vote majoritaire pour la priorit√©
        priorities = [p.priority for p in predictions]
        priority = max(set(priorities), key=priorities.count)

        return dspy.Prediction(category=category, priority=priority)
```

#### üí° Bonnes pratiques pour les modules

**‚úÖ √Ä faire :**

1. **Commencer simple** : Utilisez d'abord `Predict`, puis `ChainOfThought` si besoin
2. **Nommer clairement** : `TicketClassifier` plut√¥t que `Classifier1`
3. **Un module = une t√¢che** : Gardez les modules focalis√©s
4. **Composer progressivement** : Testez chaque module individuellement
5. **Documenter** : Ajoutez des docstrings √† vos modules personnalis√©s

**‚ùå √Ä √©viter :**

1. **Utiliser ChainOfThought partout** : Plus lent et plus co√ªteux
2. **Trop de composition** : Gardez les pipelines compr√©hensibles
3. **Oublier la validation** : V√©rifiez toujours les sorties
4. **Ne pas mesurer** : Utilisez des m√©triques pour comparer les modules

### 3. M√©triques : Mesurer la performance

## Pourquoi √©valuer ?

Jusqu'√† pr√©sent, nous avons cr√©√© des modules et observ√© leurs sorties qualitativement. Mais pour :
- **Comparer** diff√©rents modules
- **Mesurer** les am√©liorations
- **Optimiser** automatiquement (avec GEPA)

...nous avons besoin de **mesures quantitatives** : les **m√©triques**.

## Qu'est-ce qu'une m√©trique ?

Une **m√©trique** est une fonction qui prend :
- Un **exemple** avec la vraie r√©ponse (ground truth)
- Une **pr√©diction** du mod√®le
- Des param√®tres optionnels (trace, pred_name, pred_trace) pour les optimiseurs

Et retourne un **score entre 0.0 et 1.0** :
- **0.0** = Compl√®tement incorrect
- **1.0** = Parfaitement correct
- **0.5** = Partiellement correct

#### M√©trique 1 : Exact Match (correspondance exacte)

La m√©trique la plus stricte : tout doit √™tre parfait.

**Avantages :**
- ‚úÖ Simple √† comprendre
- ‚úÖ Pas d'ambigu√Øt√©
- ‚úÖ Facile √† interpr√©ter (0% ou 100%)

**Inconv√©nients :**
- ‚ùå Tr√®s stricte
- ‚ùå Ne donne pas de cr√©dit partiel
- ‚ùå Peut d√©courager si le score est trop bas

```python
def exact_match_metric(example, prediction, trace=None, pred_name=None, pred_trace=None):
    """
    Retourne 1.0 si la cat√©gorie ET la priorit√© sont correctes, 0.0 sinon.

    Args:
        example: Dict avec les vraies valeurs {'ticket': ..., 'category': ..., 'priority': ...}
        prediction: Objet Prediction avec .category et .priority
        trace: Trace d'ex√©cution (optionnel, pour GEPA)
        pred_name: Nom du predictor (optionnel, pour GEPA)
        pred_trace: Trace du predictor (optionnel, pour GEPA)

    Returns:
        float: 1.0 si exact match, 0.0 sinon
    """
    category_match = prediction.category.strip().lower() == example['category'].strip().lower()
    priority_match = prediction.priority.strip().lower() == example['priority'].strip().lower()

    return 1.0 if (category_match and priority_match) else 0.0
```

**Exemple d'utilisation :**
```python
example = {'ticket': 'Mon PC ne d√©marre pas', 'category': 'Hardware', 'priority': 'Urgent'}
prediction = dspy.Prediction(category='Hardware', priority='Urgent')

score = exact_match_metric(example, prediction)
print(f"Score: {score}")  # Output: Score: 1.0
```

#### M√©trique 2 : Partial Match (correspondance partielle)

Plus nuanc√©e : donne des points partiels si au moins un champ est correct.

**Avantages :**
- ‚úÖ Plus de nuance
- ‚úÖ Donne du cr√©dit partiel
- ‚úÖ Meilleur signal d'apprentissage

**Inconv√©nients :**
- ‚ùå Moins binaire (interpr√©tation plus complexe)

```python
def partial_match_metric(example, prediction, trace=None, pred_name=None, pred_trace=None):
    """
    Retourne un score partiel bas√© sur les champs corrects.

    Score:
    - 1.0 si cat√©gorie ET priorit√© correctes
    - 0.5 si seulement l'une des deux est correcte
    - 0.0 si les deux sont incorrectes

    Returns:
        float: Score entre 0.0 et 1.0
    """
    category_match = prediction.category.strip().lower() == example['category'].strip().lower()
    priority_match = prediction.priority.strip().lower() == example['priority'].strip().lower()

    if category_match and priority_match:
        return 1.0
    elif category_match or priority_match:
        return 0.5
    else:
        return 0.0
```

**Exemple de comparaison :**
```python
example = {'ticket': 'Mon PC ne d√©marre pas', 'category': 'Hardware', 'priority': 'Urgent'}

# Cas 1: Tout correct
pred1 = dspy.Prediction(category='Hardware', priority='Urgent')
print(f"Exact: {exact_match_metric(example, pred1)}")    # 1.0
print(f"Partial: {partial_match_metric(example, pred1)}") # 1.0

# Cas 2: Seulement la cat√©gorie correcte
pred2 = dspy.Prediction(category='Hardware', priority='Low')
print(f"Exact: {exact_match_metric(example, pred2)}")    # 0.0
print(f"Partial: {partial_match_metric(example, pred2)}") # 0.5

# Cas 3: Tout incorrect
pred3 = dspy.Prediction(category='Software', priority='Low')
print(f"Exact: {exact_match_metric(example, pred3)}")    # 0.0
print(f"Partial: {partial_match_metric(example, pred3)}") # 0.0
```

#### Fonction d'√©valuation r√©utilisable

Cr√©ons une fonction pour √©valuer n'importe quel module sur un dataset complet.

```python
def evaluate_module(module, dataset, metric):
    """
    √âvalue un module DSPy sur un dataset avec une m√©trique.

    Args:
        module: Module DSPy √† √©valuer
        dataset: Liste de dictionnaires avec 'ticket', 'category', 'priority'
        metric: Fonction de m√©trique (exact_match_metric ou partial_match_metric)

    Returns:
        float: Score moyen sur le dataset (entre 0.0 et 1.0)
    """
    scores = []

    for example in dataset:
        # Faire la pr√©diction
        prediction = module(ticket=example['ticket'])

        # Calculer le score
        score = metric(example, prediction)
        scores.append(score)

    # Retourner le score moyen
    return sum(scores) / len(scores) if scores else 0.0
```

**Exemple d'utilisation :**
```python
from src.data import get_val_examples
from src.modules import SimpleTicketClassifier

# Charger les donn√©es
val_examples = get_val_examples()

# Cr√©er le classifier
classifier = SimpleTicketClassifier()

# √âvaluer
score_exact = evaluate_module(classifier, val_examples, exact_match_metric)
score_partial = evaluate_module(classifier, val_examples, partial_match_metric)

print(f"Exact match: {score_exact:.1%}")      # ex: 65.0%
print(f"Partial match: {score_partial:.1%}")  # ex: 82.5%
```

#### üí° Bonnes pratiques pour l'√©valuation

**‚úÖ √Ä faire :**

1. **Toujours avoir un dataset de validation s√©par√©** : Ne jamais √©valuer sur les donn√©es d'entra√Ænement
2. **Utiliser plusieurs m√©triques** : Exact match + partial match donnent une vue compl√®te
3. **Tester sur des cas limites** : Tickets ambigus, tr√®s courts, tr√®s longs
4. **Documenter vos m√©triques** : Expliquez ce que signifie chaque score
5. **Comparer de mani√®re √©quitable** : M√™me dataset, m√™me m√©trique

**‚ùå √Ä √©viter :**

1. **Une seule m√©trique** : Peut ne pas capturer toute la complexit√©
2. **Dataset trop petit** : Minimum 10-15 exemples de validation
3. **√âvaluer sur le trainset** : Donnera des scores artificiellement √©lev√©s
4. **Ignorer les cas limites** : Les erreurs se cachent souvent dans les edge cases

### 4. Optimiseurs : Am√©liorer automatiquement

## Introduction : Modules vs Optimiseurs

Jusqu'√† pr√©sent, nous avons vu des **modules** (Predict, ChainOfThought, ReAct, etc.). Ces modules **ex√©cutent** des t√¢ches en interrogeant le LLM.

Les **optimiseurs**, quant √† eux, **am√©liorent** les modules en :
- Ajoutant des exemples de d√©monstration (few-shot learning)
- Optimisant les instructions (prompts)
- Ajustant les param√®tres
- S√©lectionnant les meilleures configurations

**Analogie :** Si un module est comme un employ√© qui ex√©cute des t√¢ches, un optimiseur est comme un coach qui entra√Æne l'employ√© √† s'am√©liorer.

## BootstrapFewShot : G√©n√©rer des exemples de d√©monstration

**BootstrapFewShot** est l'optimiseur le plus simple de DSPy. Il fonctionne en :

1. Ex√©cutant votre module sur les donn√©es d'entra√Ænement
2. Gardant les pr√©dictions correctes (valid√©es par votre m√©trique)
3. Utilisant ces pr√©dictions comme exemples de d√©monstration (few-shot)
4. Injectant ces exemples dans le prompt du module optimis√©

**Avantages :**
- ‚úÖ Simple √† comprendre et √† utiliser
- ‚úÖ Rapide √† ex√©cuter
- ‚úÖ Am√©lioration typique de 5-15%
- ‚úÖ Pas besoin de configuration complexe

**Inconv√©nients :**
- ‚ùå N'optimise pas les instructions
- ‚ùå Am√©lioration limit√©e compar√© √† MIPRO ou GEPA

**Quand l'utiliser :**
- Premi√®re optimisation
- Tests rapides
- Vous avez peu de temps
- Vous voulez comprendre comment fonctionne l'optimisation

**Exemple d'utilisation :**
```python
from dspy.teleprompt import BootstrapFewShot
from src.data import get_train_examples
from src.metrics import exact_match_metric

# Cr√©er votre module
classifier = SimpleTicketClassifier()

# Configurer l'optimiseur
optimizer = BootstrapFewShot(
    metric=exact_match_metric,
    max_bootstrapped_demos=4,  # Nombre max d'exemples √† g√©n√©rer
    max_labeled_demos=0,        # Pas d'exemples manuels
)

# Optimiser
train_examples = get_train_examples()
optimized_classifier = optimizer.compile(
    student=classifier,
    trainset=train_examples
)

# Le module optimis√© inclut maintenant 4 exemples de d√©monstration
```

## BootstrapFewShotWithRandomSearch

Une variante am√©lior√©e de BootstrapFewShot qui teste plusieurs combinaisons d'exemples.

**Fonctionnement :**
1. G√©n√®re plusieurs ensembles d'exemples
2. Teste diff√©rentes combinaisons
3. Garde la meilleure configuration selon la m√©trique

**Am√©lioration typique :** 8-18%

**Quand l'utiliser :** Quand vous avez un peu plus de temps que BootstrapFewShot standard.

## SignatureOptimizer

**SignatureOptimizer** se concentre uniquement sur l'optimisation des instructions de votre signature, sans ajouter d'exemples de d√©monstration.

**Fonctionnement :**
1. G√©n√®re plusieurs variantes d'instructions pour votre signature
2. Teste chaque variante
3. Garde la meilleure

**Configuration :**
```python
from dspy.teleprompt import SignatureOptimizer

optimizer = SignatureOptimizer(
    metric=exact_match_metric,
    breadth=10,  # Nombre de variantes √† g√©n√©rer
    depth=3      # Nombre d'it√©rations de raffinement
)

optimized = optimizer.compile(
    student=classifier,
    trainset=train_examples,
    valset=val_examples
)
```

**Am√©lioration typique :** 5-12%

**Quand l'utiliser :**
- Vous voulez am√©liorer vos prompts sans ajouter d'exemples
- Vous avez des contraintes de latence (les exemples ajoutent des tokens)

## MIPRO : Optimisation des instructions et exemples

**MIPRO** (Multi-prompt Instruction Proposal Optimizer) est un optimiseur plus avanc√© qui :

1. **G√©n√®re plusieurs variantes d'instructions** pour votre signature
2. **S√©lectionne les meilleurs exemples** de d√©monstration
3. **Teste diff√©rentes combinaisons** (instructions √ó exemples)
4. **Garde la meilleure configuration** selon votre m√©trique

**Avantages :**
- ‚úÖ Optimise √† la fois les instructions et les exemples
- ‚úÖ Am√©lioration typique de 10-25%
- ‚úÖ Recherche syst√©matique de la meilleure configuration

**Inconv√©nients :**
- ‚ùå Plus lent que BootstrapFewShot
- ‚ùå N√©cessite plus d'appels LLM
- ‚ùå Configuration plus complexe

**Quand l'utiliser :**
- Apr√®s avoir test√© BootstrapFewShot
- Vous voulez une meilleure performance
- Vous avez 10-20 minutes pour l'optimisation

**Configuration :**
```python
from dspy.teleprompt import MIPRO

optimizer = MIPRO(
    metric=exact_match_metric,
    num_candidates=10,      # Nombre de variantes d'instructions √† tester
    init_temperature=1.0    # Temp√©rature pour la g√©n√©ration
)

optimized = optimizer.compile(
    student=classifier,
    trainset=train_examples,
    valset=val_examples,
    num_trials=10,          # Nombre d'essais
    max_bootstrapped_demos=4
)
```

## GEPA (Genetic-Pareto Algorithm)

L'optimiseur le plus sophistiqu√©. Il combine :
- **Algorithmes g√©n√©tiques** : √âvolution de populations d'instructions
- **R√©flexion par mod√®le de langage** : Analyse des erreurs et propositions d'am√©liorations
- **Optimisation Pareto** : √âquilibre de multiples objectifs

**Am√©lioration typique :** 15 √† 30 %

**Niveaux d'optimisation :**

| Niveau   | Dur√©e      | Appels LLM | Am√©lioration | Utilisation             |
|----------|------------|------------|--------------|-------------------------|
| `light`  | 5-10 min   | ~200-400   | 10-20%       | Tests, prototypage      |
| `medium` | 10-20 min  | ~400-800   | 15-25%       | Production l√©g√®re       |
| `heavy`  | 20-40 min  | ~800-1600  | 20-30%       | Performance maximale    |

## Comparaison des optimiseurs

| Optimiseur | Ce qu'il optimise | Vitesse | Am√©lioration typique | Complexit√© | Quand l'utiliser |
|------------|------------------|---------|---------------------|------------|------------------|
| **BootstrapFewShot** | Exemples uniquement | ‚ö°‚ö°‚ö° Rapide | 5-15% | Simple | Premi√®re optimisation, tests rapides |
| **BootstrapFewShotWithRandomSearch** | Exemples (avec recherche) | ‚ö°‚ö° Moyen | 8-18% | Simple | Quand vous avez un peu plus de temps |
| **SignatureOptimizer** | Instructions uniquement | ‚ö°‚ö° Moyen | 5-12% | Moyen | Am√©liorer les prompts sans exemples |
| **MIPRO** | Instructions + exemples | ‚ö° Lent | 10-25% | Moyen | Production, bonne performance |
| **GEPA** | Instructions + exemples + r√©flexion | üêå Tr√®s lent | 15-30% | √âlev√©e | Performance maximale, t√¢ches critiques |

## Strat√©gie d'optimisation recommand√©e

1. **Phase 1 : Baseline** - Commencez sans optimisation pour avoir un point de r√©f√©rence
2. **Phase 2 : BootstrapFewShot** - Premi√®re am√©lioration rapide (5-10 minutes)
3. **Phase 3 : MIPRO** - Si les r√©sultats sont prometteurs (10-20 minutes)
4. **Phase 4 : GEPA** - Pour la performance maximale sur les t√¢ches critiques (20-40 minutes)

## Comment GEPA fonctionne

GEPA utilise une approche inspir√©e de l'√©volution biologique :

1. **Population initiale** : G√©n√®re plusieurs variantes d'instructions
2. **√âvaluation** : Teste chaque variante sur les donn√©es d'entra√Ænement
3. **S√©lection** : Garde les meilleures (front de Pareto)
4. **R√©flexion** : Un mod√®le de langage analyse les erreurs et propose des am√©liorations
5. **Mutation** : G√©n√®re de nouvelles variantes bas√©es sur la r√©flexion
6. **R√©p√©tition** : Continue jusqu'√† convergence

**Avantages de GEPA :**
- Trouve des instructions que les humains n'auraient pas imagin√©es
- Apprend de ses erreurs de mani√®re it√©rative
- √âquilibre plusieurs objectifs (pr√©cision, concision, etc.)

**Quand utiliser GEPA :**
- Vous visez la meilleure performance possible
- Vous avez suffisamment de donn√©es (20+ exemples)
- Vous avez du temps pour l'optimisation (10-30 minutes)
- La t√¢che est critique pour votre application

## Multi-mod√®les et flexibilit√©

### Introduction : Pourquoi utiliser plusieurs mod√®les ?

DSPy offre une **abstraction puissante** : votre code reste le m√™me quel que soit le mod√®le utilis√©. Vous pouvez :

1. **Changer de fournisseur** facilement (Ollama ‚Üí OpenAI ‚Üí Anthropic)
2. **Comparer les performances** de diff√©rents mod√®les
3. **Cr√©er des architectures hybrides** (mod√®le rapide pour cat√©gorie, mod√®le pr√©cis pour priorit√©)
4. **Optimiser co√ªt vs performance**

**Avantages du multi-mod√®les :**
- üí∞ **Optimisation des co√ªts** : Utilisez des mod√®les gratuits (Ollama) pour le d√©veloppement
- üéØ **Meilleure performance** : Testez plusieurs mod√®les pour trouver le meilleur
- üîí **Confidentialit√©** : Mod√®les locaux pour les donn√©es sensibles
- ‚ö° **Flexibilit√©** : Changez de mod√®le sans r√©√©crire votre code

### Configuration de diff√©rents fournisseurs

#### Ollama (local, gratuit)

Ollama permet d'ex√©cuter des mod√®les **localement** sans API key ni co√ªts.

**Mod√®les recommand√©s :**
- `llama3.1:8b` - √âquilibr√©, bon pour la plupart des t√¢ches (4.7 GB)
- `mistral:7b` - Rapide, bon pour les t√¢ches simples (4.1 GB)
- `qwen2.5:7b` - Haute qualit√©, excellent pour les t√¢ches complexes (4.7 GB)
- `gemma2:9b` - Alternative de Google, tr√®s performant (5.4 GB)

**Configuration :**
```python
import dspy

# Configurer Ollama
lm = dspy.LM(
    model='ollama_chat/llama3.1:8b',
    api_base='http://localhost:11434',
    temperature=0.0  # D√©terministe pour la classification
)

dspy.configure(lm=lm)
```

#### OpenAI (API, payant)

OpenAI propose des mod√®les tr√®s performants via API.

**Mod√®les recommand√©s :**
- `gpt-4o-mini` - Rapide et √©conomique, bon rapport qualit√©/prix
- `gpt-4o` - Haute performance, multimodal
- `gpt-4-turbo` - √âquilibr√© performance/co√ªt

**Configuration :**
```python
import dspy
import os

# Votre cl√© API OpenAI
os.environ['OPENAI_API_KEY'] = 'sk-...'

# Configurer OpenAI
lm = dspy.LM(
    model='openai/gpt-4o-mini',
    temperature=0.0
)

dspy.configure(lm=lm)
```

#### Anthropic (API, payant)

Anthropic propose les mod√®les Claude, connus pour leur qualit√© et leur s√©curit√©.

**Mod√®les recommand√©s :**
- `claude-3-5-haiku-20241022` - Rapide et √©conomique
- `claude-3-5-sonnet-20241022` - √âquilibr√©, excellent pour la plupart des t√¢ches
- `claude-3-opus-20240229` - Maximum de performance

**Configuration :**
```python
import dspy
import os

# Votre cl√© API Anthropic
os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'

# Configurer Anthropic
lm = dspy.LM(
    model='anthropic/claude-3-5-sonnet-20241022',
    temperature=0.0
)

dspy.configure(lm=lm)
```

### Comparer les performances de diff√©rents mod√®les

Cr√©ons une fonction de benchmarking pour comparer les mod√®les :

```python
import time
from src.data import get_val_examples
from src.metrics import exact_match_metric

def benchmark_model(model_config, val_examples, metric):
    """
    Benchmark un mod√®le sur un dataset de validation.

    Args:
        model_config: Configuration du mod√®le (nom, api_base, etc.)
        val_examples: Dataset de validation
        metric: M√©trique d'√©valuation

    Returns:
        dict: R√©sultats avec score, temps, et mod√®le
    """
    # Configurer le mod√®le
    lm = dspy.LM(**model_config)
    dspy.configure(lm=lm)

    # Cr√©er le classifier
    classifier = SimpleTicketClassifier()

    # Mesurer le temps
    start_time = time.time()

    # √âvaluer
    scores = []
    for example in val_examples:
        pred = classifier(ticket=example['ticket'])
        score = metric(example, pred)
        scores.append(score)

    elapsed_time = time.time() - start_time
    avg_score = sum(scores) / len(scores)

    return {
        'model': model_config['model'],
        'score': avg_score,
        'time': elapsed_time,
        'time_per_example': elapsed_time / len(val_examples)
    }

# Comparer plusieurs mod√®les Ollama
models = [
    {'model': 'ollama_chat/llama3.1:8b', 'api_base': 'http://localhost:11434'},
    {'model': 'ollama_chat/mistral:7b', 'api_base': 'http://localhost:11434'},
    {'model': 'ollama_chat/qwen2.5:7b', 'api_base': 'http://localhost:11434'},
]

val_examples = get_val_examples()
results = [benchmark_model(m, val_examples, exact_match_metric) for m in models]

# Afficher les r√©sultats
for r in results:
    print(f"{r['model']:30} | Score: {r['score']:.1%} | Temps: {r['time']:.1f}s ({r['time_per_example']:.2f}s/ex)")
```

### Architectures hybrides : utiliser diff√©rents mod√®les pour diff√©rentes t√¢ches

Une **architecture hybride** utilise diff√©rents mod√®les pour diff√©rentes parties de votre pipeline. Par exemple :

- Mod√®le **rapide et √©conomique** pour la cat√©gorisation
- Mod√®le **pr√©cis mais co√ªteux** pour la priorisation

**Avantages :**
- üí∞ **Optimisation des co√ªts** : Utiliser des mod√®les co√ªteux uniquement quand n√©cessaire
- ‚ö° **Optimisation de la vitesse** : Mod√®les rapides pour les t√¢ches simples
- üéØ **Optimisation de la qualit√©** : Mod√®les pr√©cis pour les t√¢ches critiques

**Exemple :**
```python
class HybridTicketClassifier(dspy.Module):
    """
    Classifier hybride utilisant diff√©rents mod√®les pour diff√©rentes t√¢ches.
    """

    def __init__(self, category_lm, priority_lm):
        super().__init__()
        self.category_lm = category_lm
        self.priority_lm = priority_lm

    def forward(self, ticket):
        # √âtape 1 : Cat√©gorie avec mod√®le rapide (Ollama)
        with dspy.context(lm=self.category_lm):
            category_pred = dspy.Predict(CategoryClassifier)
            category_result = category_pred(ticket=ticket)

        # √âtape 2 : Priorit√© avec mod√®le pr√©cis (OpenAI)
        with dspy.context(lm=self.priority_lm):
            priority_pred = dspy.ChainOfThought(PriorityClassifier)
            priority_result = priority_pred(
                ticket=ticket,
                category=category_result.category
            )

        return dspy.Prediction(
            category=category_result.category,
            priority=priority_result.priority
        )

# Configuration
fast_lm = dspy.LM(model='ollama_chat/llama3.1:8b', api_base='http://localhost:11434')
precise_lm = dspy.LM(model='openai/gpt-4o-mini')

# Cr√©er le classifier hybride
hybrid_classifier = HybridTicketClassifier(
    category_lm=fast_lm,
    priority_lm=precise_lm
)
```

### Guide de s√©lection de mod√®les

| Crit√®re | Ollama (local) | OpenAI | Anthropic |
|---------|---------------|--------|-----------|
| **Co√ªt** | Gratuit (mat√©riel local) | Payant √† l'usage | Payant √† l'usage |
| **Vitesse** | D√©pend du mat√©riel | Rapide (API cloud) | Rapide (API cloud) |
| **Confidentialit√©** | ‚úÖ 100% local | ‚ö†Ô∏è Donn√©es envoy√©es √† OpenAI | ‚ö†Ô∏è Donn√©es envoy√©es √† Anthropic |
| **Qualit√©** | Bonne (7-8B params) | Excellente | Excellente |
| **Disponibilit√©** | D√©pend de votre machine | Haute (99.9% uptime) | Haute (99.9% uptime) |
| **Setup** | Installation locale requise | API key uniquement | API key uniquement |

**Recommandations :**

- **D√©veloppement/tests** : Ollama (gratuit, rapide √† it√©rer)
- **Production avec donn√©es sensibles** : Ollama (confidentialit√©)
- **Production haute performance** : OpenAI ou Anthropic (qualit√© maximale)
- **Production √©conomique** : Hybride (Ollama pour les t√¢ches simples, API pour les t√¢ches complexes)

## Patterns avanc√©s (Production)

Cette section couvre des **patterns de production** pour rendre vos modules DSPy plus robustes, fiables et performants.

### Pourquoi utiliser ces patterns ?

En production, les LLMs peuvent :
- ‚ùå G√©n√©rer des sorties invalides (mauvais format, valeurs hors limites)
- ‚ùå √âchouer temporairement (timeout, rate limiting)
- ‚ùå Produire des r√©sultats incoh√©rents
- ‚ùå √ätre indisponibles (downtime API)

Les **patterns avanc√©s** permettent de :
- ‚úÖ Valider et corriger les sorties
- ‚úÖ R√©essayer automatiquement en cas d'erreur
- ‚úÖ Basculer vers un mod√®le de secours
- ‚úÖ Combiner plusieurs pr√©dictions pour plus de robustesse

### Pattern 1 : Validation

Le **pattern de validation** v√©rifie que les sorties du LLM respectent les contraintes de votre application.

**Probl√®me :**
Les LLMs peuvent g√©n√©rer :
- Des cat√©gories qui n'existent pas ("Mat√©riel" au lieu de "Hardware")
- Des priorit√©s invalides ("Tr√®s urgent" au lieu de "Urgent")
- Des formats incorrects (minuscules au lieu de majuscules)

**Solution :**
```python
class ValidatedTicketClassifier(dspy.Module):
    """
    Classifier avec validation et correction des sorties.
    """

    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(TicketClassifier)

        # Valeurs valides
        self.valid_categories = {"Hardware", "Software", "Network", "Access", "Other"}
        self.valid_priorities = {"Low", "Medium", "High", "Urgent", "Critical"}

        # Mapping pour la normalisation
        self.category_mapping = {
            "mat√©riel": "Hardware",
            "logiciel": "Software",
            "r√©seau": "Network",
            "acc√®s": "Access",
        }
        self.priority_mapping = {
            "faible": "Low",
            "moyen": "Medium",
            "haute": "High",
            "urgent": "Urgent",
            "critique": "Critical",
        }

    def _normalize(self, value, valid_set, mapping, default):
        """Normalise et valide une valeur."""
        # Nettoyer
        value = value.strip()

        # D√©j√† valide ?
        if value in valid_set:
            return value

        # Essayer en minuscules
        lower_value = value.lower()
        if lower_value in mapping:
            return mapping[lower_value]

        # Fallback : valeur par d√©faut
        return default

    def forward(self, ticket):
        # Pr√©diction
        result = self.predictor(ticket=ticket)

        # Validation et normalisation
        result.category = self._normalize(
            result.category,
            self.valid_categories,
            self.category_mapping,
            "Other"
        )
        result.priority = self._normalize(
            result.priority,
            self.valid_priorities,
            self.priority_mapping,
            "Medium"
        )

        return result
```

### Pattern 2 : Retry (r√©essayer en cas d'erreur)

Le **pattern de retry** r√©essaye automatiquement une op√©ration en cas d'√©chec temporaire.

**Solution :**
```python
import time
from typing import Optional

class RetryTicketClassifier(dspy.Module):
    """
    Classifier avec retry automatique en cas d'erreur.
    """

    def __init__(self, max_retries=3, backoff_factor=2):
        super().__init__()
        self.predictor = dspy.ChainOfThought(TicketClassifier)
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor

    def forward(self, ticket):
        last_error = None

        for attempt in range(self.max_retries):
            try:
                # Essayer la pr√©diction
                result = self.predictor(ticket=ticket)
                return result

            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    # Attendre avec backoff exponentiel
                    wait_time = self.backoff_factor ** attempt
                    time.sleep(wait_time)

        # Si tous les essais ont √©chou√©
        raise Exception(f"Failed after {self.max_retries} attempts: {last_error}")
```

### Pattern 3 : Fallback (mod√®le de secours)

Le **pattern de fallback** utilise un mod√®le de secours si le mod√®le principal √©choue.

**Solution :**
```python
class FallbackTicketClassifier(dspy.Module):
    """
    Classifier avec fallback vers un mod√®le de secours.
    """

    def __init__(self, primary_lm, fallback_lm):
        super().__init__()
        self.primary_lm = primary_lm
        self.fallback_lm = fallback_lm

    def forward(self, ticket):
        # Essayer avec le mod√®le principal
        try:
            with dspy.context(lm=self.primary_lm):
                predictor = dspy.ChainOfThought(TicketClassifier)
                result = predictor(ticket=ticket)
                result.model_used = "primary"
                return result

        except Exception as e:
            # Fallback vers le mod√®le de secours
            with dspy.context(lm=self.fallback_lm):
                predictor = dspy.Predict(TicketClassifier)
                result = predictor(ticket=ticket)
                result.model_used = "fallback"
                return result
```

### Pattern 4 : Ensemble (combiner plusieurs pr√©dictions)

Le **pattern d'ensemble** combine les pr√©dictions de plusieurs mod√®les pour am√©liorer la robustesse.

**Solution :**
```python
class EnsembleTicketClassifier(dspy.Module):
    """
    Classifier d'ensemble combinant plusieurs mod√®les.
    """

    def __init__(self, models):
        super().__init__()
        self.models = models

    def forward(self, ticket):
        predictions = []

        # Obtenir les pr√©dictions de chaque mod√®le
        for lm in self.models:
            with dspy.context(lm=lm):
                predictor = dspy.ChainOfThought(TicketClassifier)
                pred = predictor(ticket=ticket)
                predictions.append(pred)

        # Vote majoritaire pour la cat√©gorie
        categories = [p.category for p in predictions]
        category = max(set(categories), key=categories.count)

        # Vote majoritaire pour la priorit√©
        priorities = [p.priority for p in predictions]
        priority = max(set(priorities), key=priorities.count)

        return dspy.Prediction(category=category, priority=priority)
```

### Combiner les patterns

En production, vous pouvez **combiner plusieurs patterns** :

```python
class ProductionTicketClassifier(dspy.Module):
    """
    Classifier de production avec tous les patterns.
    """

    def __init__(self, primary_lm, fallback_lm, max_retries=3):
        super().__init__()
        self.primary_lm = primary_lm
        self.fallback_lm = fallback_lm
        self.max_retries = max_retries

        # Valeurs valides
        self.valid_categories = {"Hardware", "Software", "Network", "Access", "Other"}
        self.valid_priorities = {"Low", "Medium", "High", "Urgent", "Critical"}

    def _validate(self, result):
        """Valide et normalise les sorties."""
        if result.category not in self.valid_categories:
            result.category = "Other"
        if result.priority not in self.valid_priorities:
            result.priority = "Medium"
        return result

    def forward(self, ticket):
        last_error = None

        # Pattern Retry
        for attempt in range(self.max_retries):
            try:
                # Pattern Fallback
                try:
                    with dspy.context(lm=self.primary_lm):
                        predictor = dspy.ChainOfThought(TicketClassifier)
                        result = predictor(ticket=ticket)
                except Exception:
                    with dspy.context(lm=self.fallback_lm):
                        predictor = dspy.Predict(TicketClassifier)
                        result = predictor(ticket=ticket)

                # Pattern Validation
                result = self._validate(result)
                return result

            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)

        raise Exception(f"Failed after {self.max_retries} attempts: {last_error}")
```

## Guide d'utilisation rapide

### Exemple 1 : Utilisation de base

```python
# 1. Configurer DSPy avec Ollama
from src.config import configure_ollama

lm = configure_ollama()

# 2. Cr√©er un module
from src.modules import SimpleTicketClassifier

classifier = SimpleTicketClassifier()

# 3. Faire des pr√©dictions
ticket = "Mon ordinateur ne d√©marre plus, j'ai une pr√©sentation dans 1 heure"
result = classifier(ticket=ticket)

print(f"Cat√©gorie : {result.category}")
print(f"Priorit√© : {result.priority}")
```

### Exemple 2 : √âvaluation

```python
from src.evaluation import evaluate_module
from src.metrics import exact_match_metric
from src.data import get_val_examples

# √âvaluer sur l'ensemble de validation
val_examples = get_val_examples()
score = evaluate_module(classifier, val_examples, exact_match_metric)

print(f"Score : {score:.2%}")
```

### Exemple 3 : Optimisation avec BootstrapFewShot

```python
from src.optimizers import optimize_with_bootstrap
from src.data import get_train_examples

# Donn√©es d'entra√Ænement
train_examples = get_train_examples()

# Optimiser
optimized = optimize_with_bootstrap(
    classifier,
    train_examples,
    exact_match_metric,
    max_bootstrapped_demos=4
)

# √âvaluer l'am√©lioration
score_optimized = evaluate_module(optimized, val_examples, exact_match_metric)
print(f"Score optimis√© : {score_optimized:.2%}")
```

### Exemple 4 : Optimisation avec GEPA

```python
from src.gepa_utils import optimize_with_gepa
from src.config import configure_reflection_lm

# Configurer le mod√®le de r√©flexion
reflection_lm = configure_reflection_lm()

# Optimiser avec GEPA (mode l√©ger pour d√©buter)
gepa_optimized = optimize_with_gepa(
    classifier,
    train_examples,
    val_examples,
    exact_match_metric,
    reflection_lm,
    auto='light'
)

# √âvaluer
score_gepa = evaluate_module(gepa_optimized, val_examples, exact_match_metric)
print(f"Score GEPA : {score_gepa:.2%}")
```

### Exemple 5 : Ex√©cuter tous les exemples

```bash
# Exemple de base
python src/examples.py 1

# √âvaluation
python src/examples.py 2

# Optimisation
python src/examples.py 3

# GEPA
python src/examples.py 4

# Comparaison de modules
python src/examples.py 5

# Tous les exemples
python src/examples.py all
```

## Meilleures pratiques

### Donn√©es

- **Minimum 20-30 exemples** pour l'entra√Ænement
- **Exemples diversifi√©s** couvrant tous les cas d'usage
- **Ensemble de validation s√©par√©** (15-20 % des donn√©es)
- **√âtiquettes coh√©rentes** et v√©rifi√©es

### M√©triques

- **Commencer simple** : exact_match pour d√©buter
- **Ajouter des nuances** : partial_match pour plus de signal
- **Tester sur des cas limites** : exemples ambigus, courts, longs
- **Documenter clairement** : expliquer ce que signifie chaque score

### Optimisation

- **Phase 1** : Module de base sans optimisation
- **Phase 2** : BootstrapFewShot pour am√©lioration rapide
- **Phase 3** : MIPRO si les r√©sultats sont prometteurs
- **Phase 4** : GEPA pour performance maximale

### Production

- **Toujours valider** les sorties
- **Impl√©menter un r√©essai** pour les APIs
- **Avoir un plan B** (mod√®le de secours)
- **Surveiller la performance** en continu
- **Collecter les erreurs** pour am√©lioration continue

## D√©pannage

### Ollama ne d√©marre pas

```bash
# V√©rifier qu'Ollama est install√©
ollama --version

# D√©marrer Ollama
ollama serve

# V√©rifier les mod√®les disponibles
ollama list
```

### Le mod√®le n'est pas trouv√©

```bash
# T√©l√©charger le mod√®le
ollama pull llama3.1:8b
```

### Erreur de m√©moire insuffisante

- Utiliser un mod√®le plus petit : `mistral:7b`
- R√©duire `max_tokens` du reflection_lm
- Utiliser `auto='light'` au lieu de `medium` ou `heavy`
- Fermer les autres applications

### GEPA ne s'am√©liore pas

- V√©rifier la qualit√© des donn√©es (minimum 20 exemples)
- V√©rifier que la m√©trique est bien d√©finie
- Essayer un niveau plus √©lev√© (`medium` ou `heavy`)
- Augmenter le nombre d'exemples d'entra√Ænement

### Erreurs GEPA courantes

#### Erreur: `TypeError: GEPA.__init__() got an unexpected keyword argument`

**Sympt√¥me:**
```
TypeError: GEPA.__init__() got an unexpected keyword argument 'breadth'
```

**Cause:** Utilisation de param√®tres de l'ancienne API GEPA (pre-3.0)

**Solution:**
```python
# ‚ùå Ancienne API (ne fonctionne plus)
optimizer = GEPA(
    metric=my_metric,
    breadth=10,
    depth=3
)

# ‚úÖ Nouvelle API (DSPy 3.0+)
optimizer = GEPA(
    metric=my_metric,
    auto='light'  # ou 'medium' ou 'heavy'
)
```

#### Erreur: `reflection_lm` manquant

**Sympt√¥me:**
```
ValueError: reflection_lm is required for GEPA
```

**Solution:** Configurer un mod√®le de r√©flexion s√©par√©:
```python
# Configurer le mod√®le de r√©flexion
reflection_lm = dspy.LM(
    model='ollama_chat/qwen2.5:7b',
    api_base='http://localhost:11434',
    temperature=1.0,       # Important : temp√©rature √©lev√©e
    max_tokens=8000        # Important : suffisamment de tokens
)

# Utiliser avec GEPA
optimizer = GEPA(
    metric=my_metric,
    auto='light',
    reflection_lm=reflection_lm
)
```

#### GEPA est trop lent

**Solutions:**
1. Utiliser `auto='light'` au lieu de `medium` ou `heavy`
2. R√©duire le nombre d'exemples d'entra√Ænement (min 20, id√©al 30-50)
3. Utiliser un mod√®le de r√©flexion plus rapide (ex: llama3.1:8b au lieu de qwen2.5:7b)
4. V√©rifier que votre machine a suffisamment de RAM

## Conseils GEPA pour une optimisation r√©ussie

### Qualit√© des donn√©es

‚úÖ **Bonnes pratiques:**
- Minimum 15-20 exemples d'entra√Ænement (id√©alement 30-50)
- Exemples **diversifi√©s** couvrant tous les cas d'usage
- Labels **corrects** et **coh√©rents**
- Donn√©es de validation **s√©par√©es** du trainset

‚ùå **√Ä √©viter:**
- Trop peu d'exemples (<10)
- Exemples r√©p√©titifs ou tr√®s similaires
- Labels incoh√©rents ou ambigus
- Utiliser le m√™me dataset pour l'entra√Ænement et la validation

### Configuration du reflection_lm

**Param√®tres essentiels:**
- **temperature = 1.0** : Temp√©rature √©lev√©e pour encourager la cr√©ativit√©
- **max_tokens = 8000** : Suffisamment de tokens pour l'analyse et les suggestions

**Choix du mod√®le:**
- `qwen2.5:7b` - Excellent pour la r√©flexion, recommand√©
- `llama3.1:8b` - Bon √©quilibre qualit√©/vitesse
- `mistral:7b` - Plus rapide mais moins performant

### Choisir le bon niveau d'optimisation

| Niveau | Quand l'utiliser |
|--------|------------------|
| **light** | - Premi√®re utilisation de GEPA<br>- Tests et prototypage<br>- Budget temps limit√© (5-10 min)<br>- Validation du concept |
| **medium** | - Production l√©g√®re<br>- Bons r√©sultats avec light<br>- Budget mod√©r√© (10-20 min)<br>- T√¢che importante |
| **heavy** | - Performance maximale requise<br>- T√¢che critique<br>- Budget temps g√©n√©reux (20-40 min)<br>- Apr√®s succ√®s avec medium |

### Interpr√©ter les r√©sultats

**Score apr√®s GEPA:**
- **<60%** : Probl√®me probable avec les donn√©es ou la t√¢che
- **60-75%** : R√©sultats OK, peut √™tre am√©lior√©
- **75-85%** : Bons r√©sultats
- **>85%** : Excellents r√©sultats

**Si les r√©sultats ne sont pas satisfaisants:**
1. V√©rifier la qualit√© des donn√©es (labels corrects ?)
2. V√©rifier que la m√©trique mesure bien ce que vous voulez
3. Essayer un niveau plus √©lev√© (`medium` ‚Üí `heavy`)
4. Ajouter plus d'exemples d'entra√Ænement
5. Essayer un meilleur mod√®le de r√©flexion

## Checklist de mise en production

Avant de d√©ployer votre application DSPy en production, voici une checklist compl√®te :

### Donn√©es et m√©triques

- [ ] **Donn√©es d'entra√Ænement de qualit√©**
  - Au moins 30-50 exemples diversifi√©s
  - Labels v√©rifi√©s et coh√©rents
  - Couverture de tous les cas d'usage importants

- [ ] **Donn√©es de validation s√©par√©es**
  - 15-20% des donn√©es totales
  - Jamais utilis√©es pour l'entra√Ænement
  - Repr√©sentatives de la production

- [ ] **M√©triques bien d√©finies**
  - Align√©es avec les objectifs business
  - Test√©es sur des cas limites
  - Document√©es clairement

### Module et optimisation

- [ ] **Module de base test√©**
  - Fonctionne sur tous les cas d'usage
  - Performance de base mesur√©e
  - Code propre et document√©

- [ ] **Optimisation effectu√©e**
  - Au moins BootstrapFewShot test√©
  - MIPRO ou GEPA si performance critique
  - Gain de performance mesur√© et document√©

- [ ] **Module optimis√© sauvegard√©**
  - Utiliser `module.save()` pour sauvegarder
  - Version control (git) des prompts optimis√©s
  - Documentation des param√®tres d'optimisation

### Robustesse et fiabilit√©

- [ ] **Validation des sorties**
  - Pattern de validation impl√©ment√©
  - Valeurs par d√©faut d√©finies
  - Gestion des cas limites

- [ ] **Gestion des erreurs**
  - Pattern de retry impl√©ment√©
  - Pattern de fallback si n√©cessaire
  - Logging des erreurs

- [ ] **Tests complets**
  - Tests unitaires pour chaque module
  - Tests d'int√©gration
  - Tests sur des cas limites

### Performance et monitoring

- [ ] **Performance acceptable**
  - Latence mesur√©e (<2s id√©alement)
  - Throughput suffisant pour la charge attendue
  - Co√ªts estim√©s et acceptables

- [ ] **Monitoring mis en place**
  - Tracking des m√©triques de performance
  - Alertes sur les erreurs
  - Logs structur√©s et accessibles

- [ ] **Plan de rollback**
  - Possibilit√© de revenir √† la version pr√©c√©dente
  - Tests de rollback effectu√©s

### S√©curit√© et confidentialit√©

- [ ] **Donn√©es sensibles prot√©g√©es**
  - Pas de donn√©es personnelles dans les logs
  - Chiffrement si n√©cessaire
  - Conformit√© RGPD/autres r√©glementations

- [ ] **API keys s√©curis√©es**
  - Stockage s√©curis√© (variables d'environnement)
  - Rotation r√©guli√®re si possible
  - Pas de keys dans le code source

### Documentation

- [ ] **Documentation technique**
  - Architecture document√©e
  - Instructions de d√©ploiement
  - Guide de troubleshooting

- [ ] **Documentation utilisateur**
  - Cas d'usage support√©s
  - Limites connues
  - Exemples d'utilisation

## Adapter ce tutoriel √† votre cas d'usage

Ce tutoriel utilise la classification de tickets IT comme exemple, mais DSPy peut √™tre appliqu√© √† de nombreux cas d'usage. Voici comment adapter ce code √† votre probl√®me.

### D√©finir votre t√¢che

**Questions √† se poser:**
1. Quelle est mon entr√©e ? (texte, image, tableau, etc.)
2. Quelle est ma sortie attendue ? (classification, extraction, g√©n√©ration, etc.)
3. Ai-je des exemples d'entr√©e/sortie ?
4. Comment mesurer si la sortie est correcte ?

### √âtapes d'adaptation

**1. Cr√©er votre signature**
```python
class MaSignature(dspy.Signature):
    """Description claire de ma t√¢che."""

    mon_entree = dspy.InputField(desc="Description de l'entr√©e")
    ma_sortie = dspy.OutputField(desc="Description de la sortie attendue")
```

**2. Pr√©parer vos donn√©es**
```python
# Format : liste de dictionnaires
mes_donnees = [
    {'mon_entree': "exemple 1", 'ma_sortie': "r√©sultat 1"},
    {'mon_entree': "exemple 2", 'ma_sortie': "r√©sultat 2"},
    # ...
]

# S√©parer train/val (80/20)
split_idx = int(len(mes_donnees) * 0.8)
train = mes_donnees[:split_idx]
val = mes_donnees[split_idx:]
```

**3. Cr√©er votre module**
```python
class MonModule(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.ChainOfThought(MaSignature)

    def forward(self, mon_entree):
        return self.predictor(mon_entree=mon_entree)
```

**4. D√©finir votre m√©trique**
```python
def ma_metrique(example, prediction, trace=None, pred_name=None, pred_trace=None):
    """Comparer la pr√©diction avec la ground truth."""
    # Votre logique de comparaison
    correct = prediction.ma_sortie == example['ma_sortie']
    return 1.0 if correct else 0.0
```

**5. √âvaluer et optimiser**
```python
# √âvaluer
module = MonModule()
score = evaluate_module(module, val, ma_metrique)
print(f"Score baseline: {score:.1%}")

# Optimiser avec BootstrapFewShot
from dspy.teleprompt import BootstrapFewShot
optimizer = BootstrapFewShot(metric=ma_metrique)
optimized = optimizer.compile(student=module, trainset=train)

# R√©√©valuer
score_opt = evaluate_module(optimized, val, ma_metrique)
print(f"Score optimis√©: {score_opt:.1%}")
```

### Exemples de cas d'usage

| Cas d'usage | Signature | Type de module recommand√© |
|-------------|-----------|---------------------------|
| **Classification de texte** | text ‚Üí category | ChainOfThought |
| **Extraction d'informations** | document ‚Üí entities | ChainOfThought |
| **G√©n√©ration de r√©sum√©** | long_text ‚Üí summary | Predict ou ChainOfThought |
| **Question-R√©ponse** | question, context ‚Üí answer | ChainOfThought |
| **Traduction** | text, target_lang ‚Üí translation | Predict |
| **Analyse de sentiment** | review ‚Üí sentiment, score | ChainOfThought |
| **G√©n√©ration de code** | description ‚Üí code | ProgramOfThought |
| **RAG (Retrieval-Augmented Generation)** | query ‚Üí answer | ReAct avec Retrieve |

## Le pouvoir de DSPy

DSPy repr√©sente un **changement de paradigme** dans le d√©veloppement avec les LLMs :

**Avant DSPy:**
- ‚ùå Prompts √©crits manuellement et fragiles
- ‚ùå Difficile de maintenir la coh√©rence
- ‚ùå Optimisation par essai-erreur
- ‚ùå Code sp√©cifique √† chaque mod√®le

**Avec DSPy:**
- ‚úÖ Prompts optimis√©s automatiquement
- ‚úÖ Abstraction propre et maintenable
- ‚úÖ Optimisation algorithmique (GEPA, MIPRO)
- ‚úÖ Ind√©pendance du fournisseur LLM

### Principes cl√©s √† retenir

1. **Commencez simple** : Signature basique ‚Üí Module Predict ‚Üí M√©trique simple
2. **It√©rez rapidement** : Testez, mesurez, optimisez, r√©p√©tez
3. **Utilisez les optimiseurs** : BootstrapFewShot ‚Üí MIPRO ‚Üí GEPA
4. **Pensez production** : Validation, retry, fallback, monitoring
5. **Documentez tout** : Code, d√©cisions, r√©sultats, probl√®mes rencontr√©s

## Ressources

### Documentation officielle

- [DSPy Documentation](https://dspy-docs.vercel.app/)
- [DSPy GitHub](https://github.com/stanfordnlp/dspy)
- [Article DSPy](https://arxiv.org/abs/2310.03714)

### GEPA

- [Article GEPA](https://arxiv.org/abs/2507.19457)
- [GEPA GitHub](https://github.com/gepa-ai/gepa)

### Ollama

- [Documentation Ollama](https://ollama.ai/docs)
- [Biblioth√®que de mod√®les Ollama](https://ollama.ai/library)

### Communaut√©

- [Discord DSPy](https://discord.gg/dspy)
- [Discord Ollama](https://discord.gg/ollama)

## Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Signaler des bogues
- Proposer des am√©liorations
- Ajouter des exemples
- Am√©liorer la documentation

## Licence

Ce projet est distribu√© sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## Remerciements

- L'√©quipe DSPy de Stanford pour ce cadre de d√©veloppement remarquable
- Les cr√©ateurs de GEPA pour cet optimiseur sophistiqu√©
- La communaut√© Ollama pour les mod√®les locaux accessibles

---

**Bon d√©veloppement avec DSPy et GEPA !**
