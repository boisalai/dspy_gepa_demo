#!/usr/bin/env python3
"""
Script de comparaison des performances de dspy.Refine vs autres modules.

Compare les modules suivants:
- SimpleTicketClassifier (baseline)
- ValidatedClassifier (avec validation)
- RefinedTicketClassifier (avec raffinement it√©ratif)

M√©triques mesur√©es:
- Accuracy (exact match)
- Temps d'ex√©cution moyen
- Temps total
"""

import time
from typing import List, Dict
import dspy

from config import configure_ollama
from modules import SimpleTicketClassifier, ValidatedClassifier, RefinedTicketClassifier
from data import valset, CATEGORIES, PRIORITIES
from metrics import exact_match_metric, partial_match_metric


def evaluate_with_timing(
    module: dspy.Module,
    dataset: List[dict],
    module_name: str,
    verbose: bool = False
) -> Dict:
    """
    √âvalue un module avec mesure du temps d'ex√©cution.

    Args:
        module: Le module DSPy √† √©valuer
        dataset: Dataset de validation
        module_name: Nom du module pour l'affichage
        verbose: Afficher les d√©tails

    Returns:
        Dictionnaire avec les m√©triques
    """
    correct_exact = 0
    correct_partial = 0
    total = len(dataset)
    execution_times = []

    if verbose:
        print(f"\n{'='*60}")
        print(f"√âvaluation: {module_name}")
        print(f"{'='*60}")

    for i, example in enumerate(dataset, 1):
        ticket = example['ticket']
        expected_category = example['category']
        expected_priority = example['priority']

        # Mesurer le temps d'ex√©cution
        start_time = time.time()
        prediction = module(ticket=ticket)
        execution_time = time.time() - start_time
        execution_times.append(execution_time)

        # Calculer les scores
        exact_score = exact_match_metric(example, prediction)
        partial_score = partial_match_metric(example, prediction)

        correct_exact += exact_score
        correct_partial += partial_score

        if verbose:
            print(f"\n[{i}/{total}] Ticket: {ticket[:60]}...")
            print(f"  Attendu:  {expected_category} | {expected_priority}")
            print(f"  Pr√©dit:   {prediction.category} | {prediction.priority}")
            print(f"  Score exact: {exact_score:.0%}, partiel: {partial_score:.0%}")
            print(f"  Temps: {execution_time:.3f}s")

    # Calculer les statistiques
    accuracy_exact = correct_exact / total
    accuracy_partial = correct_partial / total
    avg_time = sum(execution_times) / len(execution_times)
    total_time = sum(execution_times)

    return {
        'module': module_name,
        'accuracy_exact': accuracy_exact,
        'accuracy_partial': accuracy_partial,
        'avg_time': avg_time,
        'total_time': total_time,
        'num_examples': total
    }


def print_comparison_table(results: List[Dict]):
    """Affiche un tableau comparatif des r√©sultats."""

    print("\n" + "="*90)
    print(" "*30 + "COMPARAISON DES MODULES")
    print("="*90)

    # En-t√™te
    header = f"{'Module':<30} | {'Exact Match':<12} | {'Partial Match':<14} | {'Temps Moyen':<12} | {'Temps Total':<12}"
    print(header)
    print("-"*90)

    # Donn√©es
    for result in results:
        row = (
            f"{result['module']:<30} | "
            f"{result['accuracy_exact']:>11.1%} | "
            f"{result['accuracy_partial']:>13.1%} | "
            f"{result['avg_time']:>11.3f}s | "
            f"{result['total_time']:>11.2f}s"
        )
        print(row)

    print("="*90)

    # Analyse des meilleurs
    print("\nüìä ANALYSE DES R√âSULTATS:\n")

    best_accuracy = max(results, key=lambda x: x['accuracy_exact'])
    print(f"üéØ Meilleure pr√©cision (exact match):")
    print(f"   {best_accuracy['module']}: {best_accuracy['accuracy_exact']:.1%}")

    best_partial = max(results, key=lambda x: x['accuracy_partial'])
    print(f"\nüéØ Meilleure pr√©cision (partial match):")
    print(f"   {best_partial['module']}: {best_partial['accuracy_partial']:.1%}")

    fastest = min(results, key=lambda x: x['avg_time'])
    print(f"\n‚ö° Plus rapide (temps moyen):")
    print(f"   {fastest['module']}: {fastest['avg_time']:.3f}s par pr√©diction")

    # Comparaison Refine vs Simple
    refine_result = next((r for r in results if 'Refined' in r['module']), None)
    simple_result = next((r for r in results if 'Simple' in r['module']), None)

    if refine_result and simple_result:
        print(f"\nüîÑ Impact du raffinement (Refine vs Simple):")
        accuracy_gain = (refine_result['accuracy_exact'] - simple_result['accuracy_exact']) * 100
        time_cost = (refine_result['avg_time'] / simple_result['avg_time'] - 1) * 100

        print(f"   Gain de pr√©cision: {accuracy_gain:+.1f} points de pourcentage")
        print(f"   Co√ªt en temps: {time_cost:+.1f}%")

        if accuracy_gain > 0:
            print(f"   üí° Le raffinement am√©liore la pr√©cision de {accuracy_gain:.1f}% au prix d'un temps {time_cost:.0f}% plus long")
        elif accuracy_gain == 0:
            print(f"   üí° Pas de gain de pr√©cision malgr√© {time_cost:.0f}% de temps suppl√©mentaire")
        else:
            print(f"   ‚ö†Ô∏è  La pr√©cision a diminu√© de {abs(accuracy_gain):.1f}% avec {time_cost:.0f}% de temps suppl√©mentaire")


def main():
    """Fonction principale."""

    print("üöÄ Comparaison des performances : Refine vs autres modules")
    print("="*90)

    # Configuration
    print("\nüìã Configuration:")
    lm = configure_ollama()
    print(f"   Dataset: validation set ({len(valset)} exemples)")
    print(f"   Cat√©gories: {', '.join(CATEGORIES[:3])}... ({len(CATEGORIES)} total)")
    print(f"   Priorit√©s: {', '.join(PRIORITIES[:3])}... ({len(PRIORITIES)} total)")

    # Cr√©er les modules √† comparer
    modules_to_compare = [
        ("SimpleTicketClassifier", SimpleTicketClassifier()),
        ("ValidatedClassifier", ValidatedClassifier()),
        ("RefinedTicketClassifier (N=3)", RefinedTicketClassifier(N=3, threshold=1.0)),
    ]

    # √âvaluer chaque module
    results = []
    for name, module in modules_to_compare:
        print(f"\n‚è≥ √âvaluation de {name}...")
        result = evaluate_with_timing(module, valset, name, verbose=False)
        results.append(result)
        print(f"   ‚úÖ Termin√©: {result['accuracy_exact']:.1%} exact, {result['avg_time']:.3f}s/exemple")

    # Afficher les r√©sultats
    print_comparison_table(results)

    # Recommandations
    print("\nüí° RECOMMANDATIONS:\n")

    refine_result = next((r for r in results if 'Refined' in r['module']), None)
    simple_result = next((r for r in results if 'Simple' in r['module']), None)

    if refine_result and simple_result:
        if refine_result['accuracy_exact'] > simple_result['accuracy_exact']:
            improvement = (refine_result['accuracy_exact'] - simple_result['accuracy_exact']) * 100
            print(f"‚úÖ Refine am√©liore la pr√©cision de {improvement:.1f}% points")
            print(f"   ‚Üí Utilisez Refine pour les t√¢ches critiques o√π la qualit√© prime")
            print(f"   ‚Üí Acceptez le co√ªt en temps ({refine_result['avg_time']:.2f}s vs {simple_result['avg_time']:.2f}s)")
        else:
            print(f"‚ö†Ô∏è  Refine n'am√©liore pas la pr√©cision sur ce dataset")
            print(f"   ‚Üí Le module Simple est suffisant et plus rapide")
            print(f"   ‚Üí Gardez Refine pour des cas d'usage plus complexes/ambigus")

    print("\n" + "="*90)


if __name__ == "__main__":
    main()
