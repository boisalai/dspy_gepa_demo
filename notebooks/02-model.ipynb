{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469a74a2",
   "metadata": {},
   "source": [
    "# Partie 2 : Le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104311e",
   "metadata": {},
   "source": [
    "## Configuration du modèle de langage\n",
    "\n",
    "Avant d'utiliser un module DSPy, vous devez configurer un modèle de langage (LM). DSPy prend en charge plusieurs fournisseurs, y compris les modèles locaux via Ollama et les API cloud comme Claude d'Anthropic.\n",
    "\n",
    "### Option 1 : Utiliser Ollama (local)\n",
    "\n",
    "Ollama vous permet d'exécuter des modèles de langage localement sur votre machine. Cette approche offre un contrôle total et ne nécessite pas de clé API.\n",
    "\n",
    "**Prérequis**\n",
    "- Installez Ollama depuis [ollama.ai](https://ollama.ai)\n",
    "- Téléchargez un modèle : `ollama pull llama3.1:8b`\n",
    "- Démarrez le serveur Ollama (généralement sur `http://localhost:11434`)\n",
    "\n",
    "**Configuration dans DSPy**\n",
    "\n",
    "Pour configurer un modèle Ollama, vous devez créer une instance `dspy.LM` avec le format `ollama_chat/<nom_du_modèle>` et spécifier l'URL de base de l'API, puis appliquer la configuration globalement avec `dspy.configure()`.\n",
    "\n",
    "**Modèles recommandés**\n",
    "- `llama3.1:8b` : Bon équilibre performance/qualité\n",
    "- `qwen3:latest` : Support du mode thinking/no_think\n",
    "- `mistral:latest` : Rapide et efficace\n",
    "\n",
    "### Option 2 : Utiliser Claude (Anthropic)\n",
    "\n",
    "Claude offre des capacités avancées de raisonnement et de compréhension via l'API d'Anthropic.\n",
    "\n",
    "**Prérequis**\n",
    "- Créez un compte sur [console.anthropic.com](https://console.anthropic.com)\n",
    "- Générez une clé API dans la section API Keys\n",
    "- Définissez la variable d'environnement `ANTHROPIC_API_KEY`\n",
    "\n",
    "**Configuration dans DSPy**\n",
    "\n",
    "Pour configurer Claude, vous devez d'abord définir votre clé API (idéalement via une variable d'environnement), puis créer une instance `dspy.LM` avec le modèle Claude souhaité.\n",
    "\n",
    "**Modèles disponibles**\n",
    "- `claude-3-5-sonnet-20241022` : Meilleur rapport qualité/prix\n",
    "- `claude-3-5-haiku-20241022` : Rapide et économique\n",
    "- `claude-3-opus-20240229` : Maximum de capacités\n",
    "\n",
    "**Important** : Ne committez jamais votre clé API dans le code. Utilisez plutôt un fichier `.env` ou des variables d'environnement pour gérer vos secrets en toute sécurité.\n",
    "\n",
    "### Vérifier la configuration\n",
    "\n",
    "Une fois configuré, vous pouvez vérifier que votre modèle fonctionne en appelant directement le modèle avec un message simple et en affichant la réponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ae439",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurer le modèle de langage\n",
    "lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Configurer DSPy globalement\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Tester la configuration\n",
    "response = lm(\"Bonjour, ça fonctionne ?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
