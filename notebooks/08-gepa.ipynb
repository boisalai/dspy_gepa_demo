{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurer le modèle de langage\n",
    "lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Configurer DSPy globalement\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'entraînement\n",
    "trainset = [\n",
    "    {\"ticket\": \"Mon ordinateur ne démarre plus depuis ce matin. J'ai une présentation importante dans 2 heures.\", \"category\": \"Hardware\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je n'arrive pas à me connecter à l'imprimante du 3e étage. Ça peut attendre.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le VPN ne fonctionne plus. Impossible d'accéder aux fichiers du serveur.\", \"category\": \"Network\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"J'ai oublié mon mot de passe Outlook. Je peux utiliser le webmail.\", \"category\": \"Account\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le site web affiche une erreur 500. Les clients ne peuvent plus commander!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Ma souris sans fil ne répond plus bien. Les piles sont faibles.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le système de paie ne calcule pas les heures supplémentaires. C'est la fin du mois.\", \"category\": \"Application\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"J'aimerais une mise à jour de mon logiciel Adobe quand vous aurez le temps.\", \"category\": \"Software\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le serveur de base de données est très lent. Toute la production est impactée.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Je ne reçois plus les emails. J'attends des réponses de fournisseurs.\", \"category\": \"Email\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon écran externe ne s'affiche plus. Je peux travailler sur le laptop.\", \"category\": \"Hardware\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le wifi de la salle A ne fonctionne pas. Réunion avec des externes dans 30 min.\", \"category\": \"Network\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je voudrais installer Slack pour mieux collaborer avec l'équipe.\", \"category\": \"Software\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le système de sauvegarde a échoué cette nuit selon le rapport.\", \"category\": \"Infrastructure\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon clavier a une touche qui colle. C'est gérable mais ennuyeux.\", \"category\": \"Peripherals\", \"priority\": \"Low\"}\n",
    "]\n",
    "\n",
    "# Données de validation\n",
    "valset = [\n",
    "    {\"ticket\": \"Le serveur de fichiers est inaccessible. Personne ne peut travailler.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"J'ai besoin d'accès au dossier comptabilité pour l'audit. C'est urgent.\", \"category\": \"Account\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"L'écran de mon collègue en vacances clignote. On peut attendre.\", \"category\": \"Hardware\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le CRM plante quand j'essaie d'exporter les contacts.\", \"category\": \"Application\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Je voudrais changer ma photo de profil quand vous aurez un moment.\", \"category\": \"Account\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"La vidéoconférence ne fonctionne pas. Réunion avec New York dans 10 minutes!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Mon antivirus affiche un message d'expiration mais tout fonctionne.\", \"category\": \"Software\", \"priority\": \"Medium\"}\n",
    "]\n",
    "\n",
    "# Catégories et priorités possibles\n",
    "CATEGORIES = [\"Hardware\", \"Software\", \"Network\", \"Application\", \"Infrastructure\", \"Account\", \"Email\", \"Peripherals\"]\n",
    "PRIORITIES = [\"Low\", \"Medium\", \"High\", \"Urgent\", \"Critical\"]\n",
    "\n",
    "print(f\"📊 Données chargées : {len(trainset)} entraînement, {len(valset)} validation\")\n",
    "print(f\"📦 {len(CATEGORIES)} catégories, {len(PRIORITIES)} priorités\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 7: GEPA en pratique\n",
    "\n",
    "## 7.1 Introduction à GEPA\n",
    "\n",
    "**GEPA** (Genetic-Pareto Algorithm) est l'optimiseur le plus sophistiqué de DSPy. Il combine plusieurs techniques avancées pour améliorer automatiquement vos prompts.\n",
    "\n",
    "### Qu'est-ce que GEPA?\n",
    "\n",
    "GEPA utilise une approche inspirée de l'évolution biologique :\n",
    "\n",
    "1. **🧬 Algorithmes génétiques** : Génère des \"populations\" de prompts qui évoluent\n",
    "2. **🤔 Réflexion LLM** : Utilise un LLM pour analyser les erreurs et proposer des améliorations\n",
    "3. **📊 Optimisation Pareto** : Équilibre plusieurs objectifs (précision, concision, etc.)\n",
    "4. **🔄 Itérations adaptatives** : Apprend de ses erreurs pour s'améliorer\n",
    "\n",
    "### Comment ça fonctionne?\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│ 1. POPULATION INITIALE                                  │\n",
    "│    Génère plusieurs variantes de prompts               │\n",
    "│    ↓                                                    │\n",
    "│ 2. ÉVALUATION                                          │\n",
    "│    Teste chaque variante sur les données d'entraînement│\n",
    "│    ↓                                                    │\n",
    "│ 3. SÉLECTION                                           │\n",
    "│    Garde les meilleurs (front de Pareto)               │\n",
    "│    ↓                                                    │\n",
    "│ 4. RÉFLEXION                                           │\n",
    "│    LLM analyse les erreurs et propose des améliorations│\n",
    "│    ↓                                                    │\n",
    "│ 5. MUTATION                                            │\n",
    "│    Génère de nouvelles variantes basées sur la réflexion│\n",
    "│    ↓                                                    │\n",
    "│ 6. RÉPÈTE jusqu'à convergence                          │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Pourquoi GEPA est différent?\n",
    "\n",
    "| Optimiseur | Approche | Réflexion LLM | Amélioration typique |\n",
    "|------------|----------|---------------|---------------------|\n",
    "| BootstrapFewShot | Exemples fixes | ❌ Non | 5-15% |\n",
    "| MIPRO | Variations systématiques | ❌ Non | 10-25% |\n",
    "| **GEPA** | **Évolution + Réflexion** | **✅ Oui** | **15-30%** |\n",
    "\n",
    "### Nouveautés dans DSPy 3.0+\n",
    "\n",
    "GEPA a été intégré directement dans DSPy et nécessite maintenant :\n",
    "- **reflection_lm** : Un modèle LLM dédié à l'analyse des erreurs\n",
    "- **auto** : Niveau d'optimisation ('light', 'medium', 'heavy')\n",
    "- **Métrique compatible** : Doit accepter les paramètres GEPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Configuration de GEPA\n",
    "\n",
    "### Prérequis\n",
    "\n",
    "Avant d'utiliser GEPA, vous devez avoir :\n",
    "\n",
    "1. ✅ **Données d'entraînement** : Au moins 15-20 exemples de qualité\n",
    "2. ✅ **Données de validation** : 5-10 exemples séparés pour éviter le surapprentissage\n",
    "3. ✅ **Métrique d'évaluation** : Fonction retournant un score entre 0 et 1\n",
    "4. ✅ **Module à optimiser** : Votre module DSPy de base\n",
    "5. ✅ **Temps** : 10-30 minutes selon le niveau d'optimisation\n",
    "\n",
    "### Configuration du modèle de réflexion\n",
    "\n",
    "Le **reflection_lm** est un LLM utilisé par GEPA pour analyser les erreurs et proposer des améliorations. Il est distinct du modèle principal.\n",
    "\n",
    "**Recommandations** :\n",
    "- Utiliser un modèle avec bonne capacité de raisonnement\n",
    "- Température élevée (1.0) pour plus de créativité\n",
    "- Max tokens élevé (8000+) pour des analyses détaillées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Configuration de GEPA\\n\")\n",
    "\n",
    "# 1. Configurer le modèle principal (celui qu'on optimise)\n",
    "lm_main = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm_main)\n",
    "\n",
    "# 2. Configurer le modèle de réflexion pour GEPA\n",
    "# Important: Température élevée pour plus de créativité dans l'analyse\n",
    "reflection_lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=1.0,      # Haute température = plus de créativité\n",
    "    max_tokens=8000       # Tokens élevés = analyses détaillées\n",
    ")\n",
    "\n",
    "print(\"✅ Modèle principal configuré: llama3.1:8b (temp=0.3)\")\n",
    "print(\"✅ Modèle de réflexion configuré: llama3.1:8b (temp=1.0)\")\n",
    "print()\n",
    "\n",
    "# 3. Préparer les données au format DSPy\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        ticket=ex['ticket'],\n",
    "        category=ex['category'],\n",
    "        priority=ex['priority']\n",
    "    ).with_inputs('ticket')\n",
    "    for ex in trainset\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    dspy.Example(\n",
    "        ticket=ex['ticket'],\n",
    "        category=ex['category'],\n",
    "        priority=ex['priority']\n",
    "    ).with_inputs('ticket')\n",
    "    for ex in valset\n",
    "]\n",
    "\n",
    "print(f\"✅ Données préparées:\")\n",
    "print(f\"   - Entraînement: {len(train_examples)} exemples\")\n",
    "print(f\"   - Validation: {len(val_examples)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Optimisation avec GEPA (mode 'light')\n",
    "\n",
    "Le mode **'light'** est le niveau d'optimisation le plus rapide de GEPA. Il est idéal pour :\n",
    "- Première expérimentation avec GEPA\n",
    "- Tests rapides (5-10 minutes)\n",
    "- Validation du concept\n",
    "- Ressources limitées\n",
    "\n",
    "### Niveaux d'optimisation GEPA\n",
    "\n",
    "| Niveau | Temps estimé | Appels LLM | Amélioration | Usage |\n",
    "|--------|-------------|------------|--------------|-------|\n",
    "| **light** | 5-10 min | ~200-400 | 10-20% | Tests, prototypage |\n",
    "| **medium** | 10-20 min | ~400-800 | 15-25% | Production légère |\n",
    "| **heavy** | 20-40 min | ~800-1600 | 20-30% | Maximum performance |\n",
    "\n",
    "### Exemple pratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import GEPA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🧬 OPTIMISATION GEPA - MODE 'LIGHT'\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 1. Créer le module à optimiser\n",
    "print(\"1️⃣ Création du module de base...\")\n",
    "baseline_classifier = SimpleTicketClassifier()\n",
    "\n",
    "# 2. Évaluer AVANT optimisation\n",
    "print(\"2️⃣ Évaluation AVANT optimisation...\")\n",
    "score_before = evaluate_module(baseline_classifier, val_examples, exact_match_metric)\n",
    "print(f\"   📊 Score baseline: {score_before:.2%}\\n\")\n",
    "\n",
    "# 3. Configurer l'optimiseur GEPA\n",
    "print(\"3️⃣ Configuration de l'optimiseur GEPA...\")\n",
    "optimizer = GEPA(\n",
    "    metric=exact_match_metric,\n",
    "    auto='light',                    # Mode rapide\n",
    "    reflection_lm=reflection_lm      # Modèle pour l'analyse des erreurs\n",
    ")\n",
    "print(\"   ✅ Optimiseur configuré (mode: light)\\n\")\n",
    "\n",
    "# 4. Lancer l'optimisation\n",
    "print(\"4️⃣ Lancement de l'optimisation GEPA...\")\n",
    "print(\"   ⏰ Cela va prendre 5-10 minutes avec Ollama\")\n",
    "print(\"   ☕ C'est le moment de prendre un café!\\n\")\n",
    "\n",
    "try:\n",
    "    optimized_classifier = optimizer.compile(\n",
    "        student=baseline_classifier,\n",
    "        trainset=train_examples,\n",
    "        valset=val_examples\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Optimisation GEPA terminée!\\n\")\n",
    "    \n",
    "    # 5. Évaluer APRÈS optimisation\n",
    "    print(\"5️⃣ Évaluation APRÈS optimisation...\")\n",
    "    score_after = evaluate_module(optimized_classifier, val_examples, exact_match_metric)\n",
    "    print(f\"   📊 Score optimisé: {score_after:.2%}\\n\")\n",
    "    \n",
    "    # 6. Calculer l'amélioration\n",
    "    improvement = ((score_after - score_before) / score_before) * 100 if score_before > 0 else 0\n",
    "    improvement_abs = score_after - score_before\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"📈 RÉSULTATS DE L'OPTIMISATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Score AVANT:      {score_before:.2%}\")\n",
    "    print(f\"Score APRÈS:      {score_after:.2%}\")\n",
    "    print(f\"Amélioration:     {improvement_abs:+.2%} ({improvement:+.1f}%)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Erreur lors de l'optimisation GEPA: {e}\")\n",
    "    print(\"   Vérifiez que:\")\n",
    "    print(\"   - Ollama est en cours d'exécution\")\n",
    "    print(\"   - Le modèle llama3.1:8b est téléchargé\")\n",
    "    print(\"   - Vous avez suffisamment de mémoire disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Analyser les prompts optimisés\n",
    "\n",
    "Une des forces de GEPA est qu'il **génère des prompts explicites** que vous pouvez inspecter et comprendre. Cela permet de :\n",
    "- 🔍 Voir ce que GEPA a changé\n",
    "- 📚 Apprendre comment améliorer vos prompts manuellement\n",
    "- ✅ Valider que les modifications ont du sens\n",
    "- 🎓 Comprendre pourquoi la performance s'est améliorée\n",
    "\n",
    "### Inspection des prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Inspection des prompts optimisés par GEPA\\n\")\n",
    "\n",
    "# Essayer d'accéder aux prompts optimisés\n",
    "if hasattr(optimized_classifier, 'classifier'):\n",
    "    predictor = optimized_classifier.classifier\n",
    "    \n",
    "    # 1. Signature optimisée\n",
    "    if hasattr(predictor, 'extended_signature'):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"📝 SIGNATURE OPTIMISÉE\")\n",
    "        print(\"=\" * 70)\n",
    "        sig = predictor.extended_signature\n",
    "        print(f\"Docstring: {sig.__doc__}\")\n",
    "        print()\n",
    "        \n",
    "        # Afficher les champs\n",
    "        if hasattr(sig, 'input_fields'):\n",
    "            print(\"Champs d'entrée:\")\n",
    "            for name, field in sig.input_fields.items():\n",
    "                desc = getattr(field, 'desc', 'N/A')\n",
    "                print(f\"  - {name}: {desc}\")\n",
    "        \n",
    "        if hasattr(sig, 'output_fields'):\n",
    "            print(\"\\nChamps de sortie:\")\n",
    "            for name, field in sig.output_fields.items():\n",
    "                desc = getattr(field, 'desc', 'N/A')\n",
    "                print(f\"  - {name}: {desc}\")\n",
    "        print()\n",
    "    \n",
    "    # 2. Exemples de démonstration\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"📚 EXEMPLES DE DÉMONSTRATION GÉNÉRÉS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Nombre d'exemples: {len(predictor.demos)}\\n\")\n",
    "        \n",
    "        # Afficher les 3 premiers exemples\n",
    "        for i, demo in enumerate(predictor.demos[:3], 1):\n",
    "            print(f\"Exemple {i}:\")\n",
    "            print(f\"  Ticket: {demo.ticket[:80]}...\")\n",
    "            if hasattr(demo, 'category'):\n",
    "                print(f\"  Catégorie: {demo.category}\")\n",
    "            if hasattr(demo, 'priority'):\n",
    "                print(f\"  Priorité: {demo.priority}\")\n",
    "            print()\n",
    "    \n",
    "    else:\n",
    "        print(\"ℹ️ Aucun exemple de démonstration trouvé\")\n",
    "        print(\"   (GEPA peut optimiser uniquement les instructions)\")\n",
    "    \n",
    "else:\n",
    "    print(\"ℹ️ Structure du module différente\")\n",
    "    print(\"   L'optimisation s'est concentrée sur d'autres aspects\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💡 Ce que GEPA a fait:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"✅ Analysé les erreurs sur les données d'entraînement\")\n",
    "print(\"✅ Généré des variantes de prompts\")\n",
    "print(\"✅ Utilisé la réflexion LLM pour proposer des améliorations\")\n",
    "print(\"✅ Sélectionné la meilleure configuration via Pareto\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Tester le classifier optimisé\n",
    "\n",
    "Maintenant que GEPA a optimisé notre classifier, testons-le sur de nouveaux exemples pour voir la différence.\n",
    "\n",
    "### Comparaison côte à côte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧪 Test comparatif: Baseline vs GEPA optimisé\\n\")\n",
    "\n",
    "# Exemples de test\n",
    "test_cases = [\n",
    "    {\n",
    "        'ticket': \"Mon ordinateur portable ne s'allume plus, j'ai une réunion importante dans 1h\",\n",
    "        'expected_category': 'Hardware',\n",
    "        'expected_priority': 'Urgent'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Je voudrais accès à la base de données pour faire des analyses\",\n",
    "        'expected_category': 'Account',\n",
    "        'expected_priority': 'Medium'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Le WiFi est complètement HS dans tout le bâtiment!\",\n",
    "        'expected_category': 'Network',\n",
    "        'expected_priority': 'Critical'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Mon logiciel de comptabilité plante quand j'exporte en PDF\",\n",
    "        'expected_category': 'Software',\n",
    "        'expected_priority': 'High'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Ticket':<50} | {'Attendu':<20} | {'Baseline':<20} | {'GEPA':<20}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "correct_baseline = 0\n",
    "correct_gepa = 0\n",
    "\n",
    "for test in test_cases:\n",
    "    ticket = test['ticket']\n",
    "    expected = f\"{test['expected_category']}/{test['expected_priority']}\"\n",
    "    \n",
    "    # Prédiction baseline\n",
    "    pred_baseline = baseline_classifier(ticket=ticket)\n",
    "    baseline_result = f\"{pred_baseline.category}/{pred_baseline.priority}\"\n",
    "    baseline_match = (pred_baseline.category == test['expected_category'] and \n",
    "                     pred_baseline.priority == test['expected_priority'])\n",
    "    \n",
    "    # Prédiction GEPA\n",
    "    pred_gepa = optimized_classifier(ticket=ticket)\n",
    "    gepa_result = f\"{pred_gepa.category}/{pred_gepa.priority}\"\n",
    "    gepa_match = (pred_gepa.category == test['expected_category'] and \n",
    "                 pred_gepa.priority == test['expected_priority'])\n",
    "    \n",
    "    if baseline_match:\n",
    "        correct_baseline += 1\n",
    "    if gepa_match:\n",
    "        correct_gepa += 1\n",
    "    \n",
    "    # Afficher avec indicateurs de succès\n",
    "    baseline_icon = \"✅\" if baseline_match else \"❌\"\n",
    "    gepa_icon = \"✅\" if gepa_match else \"❌\"\n",
    "    \n",
    "    print(f\"{ticket[:48]:<50} | {expected:<20} | {baseline_icon} {baseline_result:<17} | {gepa_icon} {gepa_result:<17}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Précision sur les exemples de test:\")\n",
    "print(f\"  Baseline: {correct_baseline}/{len(test_cases)} ({correct_baseline/len(test_cases)*100:.0f}%)\")\n",
    "print(f\"  GEPA:     {correct_gepa}/{len(test_cases)} ({correct_gepa/len(test_cases)*100:.0f}%)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Conseils pour optimiser avec GEPA\n",
    "\n",
    "### 7.6.1 Qualité des données\n",
    "\n",
    "La qualité de l'optimisation GEPA dépend **fortement** de vos données :\n",
    "\n",
    "✅ **Bonnes pratiques** :\n",
    "- Minimum 15-20 exemples d'entraînement (idéalement 30-50)\n",
    "- Exemples **diversifiés** couvrant tous les cas d'usage\n",
    "- Labels **corrects** et **cohérents**\n",
    "- Données de validation **séparées** du trainset\n",
    "\n",
    "❌ **À éviter** :\n",
    "- Trop peu d'exemples (<10)\n",
    "- Exemples répétitifs ou très similaires\n",
    "- Labels incohérents ou ambigus\n",
    "- Utiliser les mêmes données pour train et validation\n",
    "\n",
    "### 7.6.2 Choix de la métrique\n",
    "\n",
    "Votre **métrique détermine ce que GEPA optimise** :\n",
    "\n",
    "```python\n",
    "# Métrique stricte (tout ou rien)\n",
    "def exact_match(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    return 1.0 if (prediction.category == example.category and \n",
    "                   prediction.priority == example.priority) else 0.0\n",
    "\n",
    "# Métrique avec crédit partiel (souvent meilleure pour GEPA)\n",
    "def partial_match(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    category_match = prediction.category == example.category\n",
    "    priority_match = prediction.priority == example.priority\n",
    "    \n",
    "    if category_match and priority_match:\n",
    "        return 1.0\n",
    "    elif category_match:\n",
    "        return 0.7  # Catégorie plus importante\n",
    "    elif priority_match:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "```\n",
    "\n",
    "💡 **Conseil** : Les métriques avec crédit partiel donnent généralement de meilleurs résultats car elles fournissent plus de signal d'apprentissage à GEPA.\n",
    "\n",
    "### 7.6.3 Choix du niveau d'optimisation\n",
    "\n",
    "| Situation | Niveau recommandé | Raison |\n",
    "|-----------|------------------|---------|\n",
    "| Première expérimentation | **light** | Test rapide du concept |\n",
    "| Prototype pour démo | **light** | Balance vitesse/performance |\n",
    "| Application production (non-critique) | **medium** | Bon compromis |\n",
    "| Application production (critique) | **heavy** | Maximum de performance |\n",
    "| Recherche / benchmark | **heavy** | Explorer les limites |\n",
    "\n",
    "### 7.6.4 Configuration du modèle de réflexion\n",
    "\n",
    "Le **reflection_lm** influence la qualité des améliorations :\n",
    "\n",
    "✅ **Bonnes pratiques** :\n",
    "- Utiliser un modèle avec bonne capacité de raisonnement\n",
    "- Température élevée (0.8-1.2) pour la créativité\n",
    "- Max tokens élevé (6000-10000) pour analyses détaillées\n",
    "- Peut être le même modèle que le modèle principal\n",
    "\n",
    "❌ **À éviter** :\n",
    "- Modèles trop petits (<7B paramètres)\n",
    "- Température trop basse (<0.5)\n",
    "- Max tokens trop faible (<4000)\n",
    "\n",
    "### 7.6.5 Éviter le surapprentissage\n",
    "\n",
    "GEPA peut **surapprendre** sur les données d'entraînement :\n",
    "\n",
    "✅ **Prévention** :\n",
    "- Toujours avoir un **valset séparé**\n",
    "- Valider sur de **nouvelles données** après optimisation\n",
    "- Comparer les scores train vs validation\n",
    "- Si grand écart : vos données ne sont pas assez diversifiées\n",
    "\n",
    "```python\n",
    "# Bon: Évaluation sur données séparées\n",
    "score_train = evaluate_module(optimized, train_examples, metric)\n",
    "score_val = evaluate_module(optimized, val_examples, metric)\n",
    "\n",
    "if score_train - score_val > 0.2:\n",
    "    print(\"⚠️ Possible surapprentissage!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Troubleshooting et erreurs courantes\n",
    "\n",
    "### Erreur 1: `TypeError: GEPA.__init__() got an unexpected keyword argument`\n",
    "\n",
    "**Symptôme** :\n",
    "```\n",
    "TypeError: GEPA.__init__() got an unexpected keyword argument 'breadth'\n",
    "```\n",
    "\n",
    "**Cause** : Utilisation de paramètres de l'ancienne API GEPA (pre-3.0)\n",
    "\n",
    "**Solution** :\n",
    "```python\n",
    "# ❌ Ancienne API (ne fonctionne plus)\n",
    "optimizer = GEPA(\n",
    "    metric=my_metric,\n",
    "    breadth=10,\n",
    "    depth=3\n",
    ")\n",
    "\n",
    "# ✅ Nouvelle API (DSPy 3.0+)\n",
    "optimizer = GEPA(\n",
    "    metric=my_metric,\n",
    "    auto='light',  # ou 'medium', 'heavy'\n",
    "    reflection_lm=reflection_lm\n",
    ")\n",
    "```\n",
    "\n",
    "### Erreur 2: `TypeError: metric() missing required positional argument`\n",
    "\n",
    "**Symptôme** :\n",
    "```\n",
    "TypeError: metric() missing 2 required positional arguments: 'pred_name' and 'pred_trace'\n",
    "```\n",
    "\n",
    "**Cause** : Métrique pas compatible avec l'API GEPA\n",
    "\n",
    "**Solution** : Ajouter les paramètres optionnels à votre métrique\n",
    "```python\n",
    "# ❌ Ancienne signature\n",
    "def my_metric(example, prediction):\n",
    "    return 1.0 if prediction.category == example.category else 0.0\n",
    "\n",
    "# ✅ Nouvelle signature (compatible GEPA)\n",
    "def my_metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    return 1.0 if prediction.category == example.category else 0.0\n",
    "```\n",
    "\n",
    "### Erreur 3: Ollama timeout ou erreur de connexion\n",
    "\n",
    "**Symptôme** :\n",
    "```\n",
    "ConnectionError: Failed to connect to Ollama\n",
    "```\n",
    "\n",
    "**Causes possibles** :\n",
    "1. Ollama n'est pas démarré\n",
    "2. Modèle non téléchargé\n",
    "3. Mémoire insuffisante\n",
    "\n",
    "**Solutions** :\n",
    "```bash\n",
    "# 1. Vérifier qu'Ollama tourne\n",
    "ollama list\n",
    "\n",
    "# 2. Démarrer Ollama si nécessaire\n",
    "ollama serve\n",
    "\n",
    "# 3. Télécharger le modèle\n",
    "ollama pull llama3.1:8b\n",
    "\n",
    "# 4. Vérifier la mémoire disponible\n",
    "# GEPA + Ollama nécessite ~8-12 GB RAM\n",
    "```\n",
    "\n",
    "### Erreur 4: GEPA ne s'améliore pas\n",
    "\n",
    "**Symptôme** : Score après optimisation ≈ score avant\n",
    "\n",
    "**Causes possibles** :\n",
    "1. Données d'entraînement insuffisantes ou de mauvaise qualité\n",
    "2. Métrique mal définie\n",
    "3. Tâche trop difficile pour le modèle\n",
    "4. Module déjà bien optimisé\n",
    "\n",
    "**Solutions** :\n",
    "```python\n",
    "# Vérifier la qualité des données\n",
    "print(f\"Nombre d'exemples train: {len(trainset)}\")\n",
    "print(f\"Nombre d'exemples val: {len(valset)}\")\n",
    "\n",
    "# Vérifier la métrique\n",
    "for ex in trainset[:5]:\n",
    "    pred = baseline(ticket=ex['ticket'])\n",
    "    score = metric(ex, pred)\n",
    "    print(f\"Score: {score} | Pred: {pred.category}/{pred.priority} | Truth: {ex['category']}/{ex['priority']}\")\n",
    "\n",
    "# Essayer un niveau plus élevé\n",
    "optimizer = GEPA(\n",
    "    metric=metric,\n",
    "    auto='heavy',  # au lieu de 'light'\n",
    "    reflection_lm=reflection_lm\n",
    ")\n",
    "```\n",
    "\n",
    "### Erreur 5: Out of Memory (OOM)\n",
    "\n",
    "**Symptôme** : Ollama ou Python crash avec erreur de mémoire\n",
    "\n",
    "**Solutions** :\n",
    "1. Utiliser un modèle plus petit : `mistral:7b` au lieu de `llama3.1:8b`\n",
    "2. Réduire `max_tokens` du reflection_lm\n",
    "3. Utiliser `auto='light'` au lieu de `medium` ou `heavy`\n",
    "4. Fermer les autres applications\n",
    "5. Utiliser une API cloud au lieu d'Ollama local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Résumé de la Partie 7\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **GEPA : L'optimiseur le plus sophistiqué**\n",
    "   - Combine algorithmes génétiques + réflexion LLM + Pareto\n",
    "   - Amélioration typique : 15-30%\n",
    "   - Nécessite un modèle de réflexion (reflection_lm)\n",
    "\n",
    "2. **Configuration essentielle**\n",
    "   - `auto` : 'light', 'medium', ou 'heavy'\n",
    "   - `reflection_lm` : Modèle pour analyser les erreurs (temp=1.0, max_tokens=8000)\n",
    "   - `metric` : Doit accepter les paramètres GEPA (trace, pred_name, pred_trace)\n",
    "\n",
    "3. **Niveaux d'optimisation**\n",
    "   - **light** : 5-10 min, ~200-400 appels LLM, 10-20% amélioration\n",
    "   - **medium** : 10-20 min, ~400-800 appels LLM, 15-25% amélioration\n",
    "   - **heavy** : 20-40 min, ~800-1600 appels LLM, 20-30% amélioration\n",
    "\n",
    "4. **Inspection des résultats**\n",
    "   - Voir les prompts optimisés\n",
    "   - Comprendre les changements\n",
    "   - Valider les améliorations\n",
    "\n",
    "5. **Bonnes pratiques**\n",
    "   - Données : 15-20+ exemples diversifiés\n",
    "   - Métrique : Crédit partiel souvent meilleur\n",
    "   - Validation : Toujours sur données séparées\n",
    "   - Modèle : ≥7B paramètres recommandé\n",
    "\n",
    "6. **Troubleshooting**\n",
    "   - Erreurs d'API : Vérifier la signature de la métrique\n",
    "   - Pas d'amélioration : Vérifier qualité des données\n",
    "   - OOM : Réduire niveau ou utiliser modèle plus petit\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "- ✅ **GEPA est puissant** mais nécessite temps et ressources\n",
    "- ✅ **Commencer avec 'light'** pour tester le concept\n",
    "- ✅ **Qualité des données = qualité des résultats**\n",
    "- ✅ **Toujours valider** sur données séparées\n",
    "- ✅ **Inspecter les prompts** pour comprendre les améliorations\n",
    "\n",
    "### Quand utiliser GEPA?\n",
    "\n",
    "| Situation | GEPA? | Alternative |\n",
    "|-----------|-------|-------------|\n",
    "| Prototypage rapide | ❌ Non | Module simple |\n",
    "| Tests initiaux | ❌ Non | BootstrapFewShot |\n",
    "| Application production (non-critique) | ⚠️ Peut-être | MIPRO |\n",
    "| Application production (critique) | ✅ Oui | GEPA medium/heavy |\n",
    "| Recherche / Maximum performance | ✅ Oui | GEPA heavy |\n",
    "| Ressources limitées | ❌ Non | BootstrapFewShot |\n",
    "\n",
    "### Workflow recommandé\n",
    "\n",
    "```python\n",
    "# Phase 1: Baseline\n",
    "classifier = SimpleTicketClassifier()\n",
    "score_baseline = evaluate(classifier, valset, metric)\n",
    "\n",
    "# Phase 2: Optimisation simple\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "optimizer_simple = BootstrapFewShot(metric=metric)\n",
    "classifier_simple = optimizer_simple.compile(classifier, trainset)\n",
    "score_simple = evaluate(classifier_simple, valset, metric)\n",
    "\n",
    "# Phase 3: GEPA (si amélioration justifie le temps)\n",
    "if score_simple < target_score:\n",
    "    from dspy.teleprompt import GEPA\n",
    "    optimizer_gepa = GEPA(metric=metric, auto='light', reflection_lm=reflection_lm)\n",
    "    classifier_gepa = optimizer_gepa.compile(classifier, trainset, valset)\n",
    "    score_gepa = evaluate(classifier_gepa, valset, metric)\n",
    "    \n",
    "    # Phase 4: GEPA heavy si nécessaire\n",
    "    if score_gepa < target_score:\n",
    "        optimizer_heavy = GEPA(metric=metric, auto='heavy', reflection_lm=reflection_lm)\n",
    "        classifier_final = optimizer_heavy.compile(classifier, trainset, valset)\n",
    "```\n",
    "\n",
    "### Ressources additionnelles\n",
    "\n",
    "- 📄 [Paper GEPA (arXiv)](https://arxiv.org/abs/2507.19457)\n",
    "- 💻 [GitHub GEPA](https://github.com/gepa-ai/gepa)\n",
    "- 📖 [Documentation DSPy](https://dspy-docs.vercel.app/)\n",
    "\n",
    "### Prochaines étapes\n",
    "\n",
    "- **Partie 8** : Conclusion et mise en production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-gepa-demo (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
