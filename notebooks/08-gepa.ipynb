{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration et pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurer le mod√®le de langage\n",
    "lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Configurer DSPy globalement\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es d'entra√Ænement\n",
    "trainset = [\n",
    "    {\"ticket\": \"Mon ordinateur ne d√©marre plus depuis ce matin. J'ai une pr√©sentation importante dans 2 heures.\", \"category\": \"Hardware\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je n'arrive pas √† me connecter √† l'imprimante du 3e √©tage. √áa peut attendre.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le VPN ne fonctionne plus. Impossible d'acc√©der aux fichiers du serveur.\", \"category\": \"Network\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"J'ai oubli√© mon mot de passe Outlook. Je peux utiliser le webmail.\", \"category\": \"Account\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le site web affiche une erreur 500. Les clients ne peuvent plus commander!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Ma souris sans fil ne r√©pond plus bien. Les piles sont faibles.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le syst√®me de paie ne calcule pas les heures suppl√©mentaires. C'est la fin du mois.\", \"category\": \"Application\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"J'aimerais une mise √† jour de mon logiciel Adobe quand vous aurez le temps.\", \"category\": \"Software\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le serveur de base de donn√©es est tr√®s lent. Toute la production est impact√©e.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Je ne re√ßois plus les emails. J'attends des r√©ponses de fournisseurs.\", \"category\": \"Email\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon √©cran externe ne s'affiche plus. Je peux travailler sur le laptop.\", \"category\": \"Hardware\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le wifi de la salle A ne fonctionne pas. R√©union avec des externes dans 30 min.\", \"category\": \"Network\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je voudrais installer Slack pour mieux collaborer avec l'√©quipe.\", \"category\": \"Software\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le syst√®me de sauvegarde a √©chou√© cette nuit selon le rapport.\", \"category\": \"Infrastructure\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon clavier a une touche qui colle. C'est g√©rable mais ennuyeux.\", \"category\": \"Peripherals\", \"priority\": \"Low\"}\n",
    "]\n",
    "\n",
    "# Donn√©es de validation\n",
    "valset = [\n",
    "    {\"ticket\": \"Le serveur de fichiers est inaccessible. Personne ne peut travailler.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"J'ai besoin d'acc√®s au dossier comptabilit√© pour l'audit. C'est urgent.\", \"category\": \"Account\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"L'√©cran de mon coll√®gue en vacances clignote. On peut attendre.\", \"category\": \"Hardware\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le CRM plante quand j'essaie d'exporter les contacts.\", \"category\": \"Application\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Je voudrais changer ma photo de profil quand vous aurez un moment.\", \"category\": \"Account\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"La vid√©oconf√©rence ne fonctionne pas. R√©union avec New York dans 10 minutes!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Mon antivirus affiche un message d'expiration mais tout fonctionne.\", \"category\": \"Software\", \"priority\": \"Medium\"}\n",
    "]\n",
    "\n",
    "# Cat√©gories et priorit√©s possibles\n",
    "CATEGORIES = [\"Hardware\", \"Software\", \"Network\", \"Application\", \"Infrastructure\", \"Account\", \"Email\", \"Peripherals\"]\n",
    "PRIORITIES = [\"Low\", \"Medium\", \"High\", \"Urgent\", \"Critical\"]\n",
    "\n",
    "print(f\"üìä Donn√©es charg√©es : {len(trainset)} entra√Ænement, {len(valset)} validation\")\n",
    "print(f\"üì¶ {len(CATEGORIES)} cat√©gories, {len(PRIORITIES)} priorit√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 7: GEPA en pratique\n",
    "\n",
    "## 7.1 Introduction √† GEPA\n",
    "\n",
    "**GEPA** (Genetic-Pareto Algorithm) est l'optimiseur le plus sophistiqu√© de DSPy. Il combine plusieurs techniques avanc√©es pour am√©liorer automatiquement vos prompts.\n",
    "\n",
    "### Qu'est-ce que GEPA?\n",
    "\n",
    "GEPA utilise une approche inspir√©e de l'√©volution biologique :\n",
    "\n",
    "1. **üß¨ Algorithmes g√©n√©tiques** : G√©n√®re des \"populations\" de prompts qui √©voluent\n",
    "2. **ü§î R√©flexion LLM** : Utilise un LLM pour analyser les erreurs et proposer des am√©liorations\n",
    "3. **üìä Optimisation Pareto** : √âquilibre plusieurs objectifs (pr√©cision, concision, etc.)\n",
    "4. **üîÑ It√©rations adaptatives** : Apprend de ses erreurs pour s'am√©liorer\n",
    "\n",
    "### Comment √ßa fonctionne?\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ 1. POPULATION INITIALE                                  ‚îÇ\n",
    "‚îÇ    G√©n√®re plusieurs variantes de prompts               ‚îÇ\n",
    "‚îÇ    ‚Üì                                                    ‚îÇ\n",
    "‚îÇ 2. √âVALUATION                                          ‚îÇ\n",
    "‚îÇ    Teste chaque variante sur les donn√©es d'entra√Ænement‚îÇ\n",
    "‚îÇ    ‚Üì                                                    ‚îÇ\n",
    "‚îÇ 3. S√âLECTION                                           ‚îÇ\n",
    "‚îÇ    Garde les meilleurs (front de Pareto)               ‚îÇ\n",
    "‚îÇ    ‚Üì                                                    ‚îÇ\n",
    "‚îÇ 4. R√âFLEXION                                           ‚îÇ\n",
    "‚îÇ    LLM analyse les erreurs et propose des am√©liorations‚îÇ\n",
    "‚îÇ    ‚Üì                                                    ‚îÇ\n",
    "‚îÇ 5. MUTATION                                            ‚îÇ\n",
    "‚îÇ    G√©n√®re de nouvelles variantes bas√©es sur la r√©flexion‚îÇ\n",
    "‚îÇ    ‚Üì                                                    ‚îÇ\n",
    "‚îÇ 6. R√âP√àTE jusqu'√† convergence                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Pourquoi GEPA est diff√©rent?\n",
    "\n",
    "| Optimiseur | Approche | R√©flexion LLM | Am√©lioration typique |\n",
    "|------------|----------|---------------|---------------------|\n",
    "| BootstrapFewShot | Exemples fixes | ‚ùå Non | 5-15% |\n",
    "| MIPRO | Variations syst√©matiques | ‚ùå Non | 10-25% |\n",
    "| **GEPA** | **√âvolution + R√©flexion** | **‚úÖ Oui** | **15-30%** |\n",
    "\n",
    "### Nouveaut√©s dans DSPy 3.0+\n",
    "\n",
    "GEPA a √©t√© int√©gr√© directement dans DSPy et n√©cessite maintenant :\n",
    "- **reflection_lm** : Un mod√®le LLM d√©di√© √† l'analyse des erreurs\n",
    "- **auto** : Niveau d'optimisation ('light', 'medium', 'heavy')\n",
    "- **M√©trique compatible** : Doit accepter les param√®tres GEPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Configuration de GEPA\n",
    "\n",
    "### Pr√©requis\n",
    "\n",
    "Avant d'utiliser GEPA, vous devez avoir :\n",
    "\n",
    "1. ‚úÖ **Donn√©es d'entra√Ænement** : Au moins 15-20 exemples de qualit√©\n",
    "2. ‚úÖ **Donn√©es de validation** : 5-10 exemples s√©par√©s pour √©viter le surapprentissage\n",
    "3. ‚úÖ **M√©trique d'√©valuation** : Fonction retournant un score entre 0 et 1\n",
    "4. ‚úÖ **Module √† optimiser** : Votre module DSPy de base\n",
    "5. ‚úÖ **Temps** : 10-30 minutes selon le niveau d'optimisation\n",
    "\n",
    "### Configuration du mod√®le de r√©flexion\n",
    "\n",
    "Le **reflection_lm** est un LLM utilis√© par GEPA pour analyser les erreurs et proposer des am√©liorations. Il est distinct du mod√®le principal.\n",
    "\n",
    "**Recommandations** :\n",
    "- Utiliser un mod√®le avec bonne capacit√© de raisonnement\n",
    "- Temp√©rature √©lev√©e (1.0) pour plus de cr√©ativit√©\n",
    "- Max tokens √©lev√© (8000+) pour des analyses d√©taill√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Configuration de GEPA\\n\")\n",
    "\n",
    "# 1. Configurer le mod√®le principal (celui qu'on optimise)\n",
    "lm_main = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "dspy.configure(lm=lm_main)\n",
    "\n",
    "# 2. Configurer le mod√®le de r√©flexion pour GEPA\n",
    "# Important: Temp√©rature √©lev√©e pour plus de cr√©ativit√© dans l'analyse\n",
    "reflection_lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=1.0,      # Haute temp√©rature = plus de cr√©ativit√©\n",
    "    max_tokens=8000       # Tokens √©lev√©s = analyses d√©taill√©es\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le principal configur√©: llama3.1:8b (temp=0.3)\")\n",
    "print(\"‚úÖ Mod√®le de r√©flexion configur√©: llama3.1:8b (temp=1.0)\")\n",
    "print()\n",
    "\n",
    "# 3. Pr√©parer les donn√©es au format DSPy\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        ticket=ex['ticket'],\n",
    "        category=ex['category'],\n",
    "        priority=ex['priority']\n",
    "    ).with_inputs('ticket')\n",
    "    for ex in trainset\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    dspy.Example(\n",
    "        ticket=ex['ticket'],\n",
    "        category=ex['category'],\n",
    "        priority=ex['priority']\n",
    "    ).with_inputs('ticket')\n",
    "    for ex in valset\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√©par√©es:\")\n",
    "print(f\"   - Entra√Ænement: {len(train_examples)} exemples\")\n",
    "print(f\"   - Validation: {len(val_examples)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Optimisation avec GEPA (mode 'light')\n",
    "\n",
    "Le mode **'light'** est le niveau d'optimisation le plus rapide de GEPA. Il est id√©al pour :\n",
    "- Premi√®re exp√©rimentation avec GEPA\n",
    "- Tests rapides (5-10 minutes)\n",
    "- Validation du concept\n",
    "- Ressources limit√©es\n",
    "\n",
    "### Niveaux d'optimisation GEPA\n",
    "\n",
    "| Niveau | Temps estim√© | Appels LLM | Am√©lioration | Usage |\n",
    "|--------|-------------|------------|--------------|-------|\n",
    "| **light** | 5-10 min | ~200-400 | 10-20% | Tests, prototypage |\n",
    "| **medium** | 10-20 min | ~400-800 | 15-25% | Production l√©g√®re |\n",
    "| **heavy** | 20-40 min | ~800-1600 | 20-30% | Maximum performance |\n",
    "\n",
    "### Exemple pratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import GEPA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üß¨ OPTIMISATION GEPA - MODE 'LIGHT'\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# 1. Cr√©er le module √† optimiser\n",
    "print(\"1Ô∏è‚É£ Cr√©ation du module de base...\")\n",
    "baseline_classifier = SimpleTicketClassifier()\n",
    "\n",
    "# 2. √âvaluer AVANT optimisation\n",
    "print(\"2Ô∏è‚É£ √âvaluation AVANT optimisation...\")\n",
    "score_before = evaluate_module(baseline_classifier, val_examples, exact_match_metric)\n",
    "print(f\"   üìä Score baseline: {score_before:.2%}\\n\")\n",
    "\n",
    "# 3. Configurer l'optimiseur GEPA\n",
    "print(\"3Ô∏è‚É£ Configuration de l'optimiseur GEPA...\")\n",
    "optimizer = GEPA(\n",
    "    metric=exact_match_metric,\n",
    "    auto='light',                    # Mode rapide\n",
    "    reflection_lm=reflection_lm      # Mod√®le pour l'analyse des erreurs\n",
    ")\n",
    "print(\"   ‚úÖ Optimiseur configur√© (mode: light)\\n\")\n",
    "\n",
    "# 4. Lancer l'optimisation\n",
    "print(\"4Ô∏è‚É£ Lancement de l'optimisation GEPA...\")\n",
    "print(\"   ‚è∞ Cela va prendre 5-10 minutes avec Ollama\")\n",
    "print(\"   ‚òï C'est le moment de prendre un caf√©!\\n\")\n",
    "\n",
    "try:\n",
    "    optimized_classifier = optimizer.compile(\n",
    "        student=baseline_classifier,\n",
    "        trainset=train_examples,\n",
    "        valset=val_examples\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Optimisation GEPA termin√©e!\\n\")\n",
    "    \n",
    "    # 5. √âvaluer APR√àS optimisation\n",
    "    print(\"5Ô∏è‚É£ √âvaluation APR√àS optimisation...\")\n",
    "    score_after = evaluate_module(optimized_classifier, val_examples, exact_match_metric)\n",
    "    print(f\"   üìä Score optimis√©: {score_after:.2%}\\n\")\n",
    "    \n",
    "    # 6. Calculer l'am√©lioration\n",
    "    improvement = ((score_after - score_before) / score_before) * 100 if score_before > 0 else 0\n",
    "    improvement_abs = score_after - score_before\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìà R√âSULTATS DE L'OPTIMISATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Score AVANT:      {score_before:.2%}\")\n",
    "    print(f\"Score APR√àS:      {score_after:.2%}\")\n",
    "    print(f\"Am√©lioration:     {improvement_abs:+.2%} ({improvement:+.1f}%)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur lors de l'optimisation GEPA: {e}\")\n",
    "    print(\"   V√©rifiez que:\")\n",
    "    print(\"   - Ollama est en cours d'ex√©cution\")\n",
    "    print(\"   - Le mod√®le llama3.1:8b est t√©l√©charg√©\")\n",
    "    print(\"   - Vous avez suffisamment de m√©moire disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Analyser les prompts optimis√©s\n",
    "\n",
    "Une des forces de GEPA est qu'il **g√©n√®re des prompts explicites** que vous pouvez inspecter et comprendre. Cela permet de :\n",
    "- üîç Voir ce que GEPA a chang√©\n",
    "- üìö Apprendre comment am√©liorer vos prompts manuellement\n",
    "- ‚úÖ Valider que les modifications ont du sens\n",
    "- üéì Comprendre pourquoi la performance s'est am√©lior√©e\n",
    "\n",
    "### Inspection des prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Inspection des prompts optimis√©s par GEPA\\n\")\n",
    "\n",
    "# Essayer d'acc√©der aux prompts optimis√©s\n",
    "if hasattr(optimized_classifier, 'classifier'):\n",
    "    predictor = optimized_classifier.classifier\n",
    "    \n",
    "    # 1. Signature optimis√©e\n",
    "    if hasattr(predictor, 'extended_signature'):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"üìù SIGNATURE OPTIMIS√âE\")\n",
    "        print(\"=\" * 70)\n",
    "        sig = predictor.extended_signature\n",
    "        print(f\"Docstring: {sig.__doc__}\")\n",
    "        print()\n",
    "        \n",
    "        # Afficher les champs\n",
    "        if hasattr(sig, 'input_fields'):\n",
    "            print(\"Champs d'entr√©e:\")\n",
    "            for name, field in sig.input_fields.items():\n",
    "                desc = getattr(field, 'desc', 'N/A')\n",
    "                print(f\"  - {name}: {desc}\")\n",
    "        \n",
    "        if hasattr(sig, 'output_fields'):\n",
    "            print(\"\\nChamps de sortie:\")\n",
    "            for name, field in sig.output_fields.items():\n",
    "                desc = getattr(field, 'desc', 'N/A')\n",
    "                print(f\"  - {name}: {desc}\")\n",
    "        print()\n",
    "    \n",
    "    # 2. Exemples de d√©monstration\n",
    "    if hasattr(predictor, 'demos') and predictor.demos:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"üìö EXEMPLES DE D√âMONSTRATION G√âN√âR√âS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Nombre d'exemples: {len(predictor.demos)}\\n\")\n",
    "        \n",
    "        # Afficher les 3 premiers exemples\n",
    "        for i, demo in enumerate(predictor.demos[:3], 1):\n",
    "            print(f\"Exemple {i}:\")\n",
    "            print(f\"  Ticket: {demo.ticket[:80]}...\")\n",
    "            if hasattr(demo, 'category'):\n",
    "                print(f\"  Cat√©gorie: {demo.category}\")\n",
    "            if hasattr(demo, 'priority'):\n",
    "                print(f\"  Priorit√©: {demo.priority}\")\n",
    "            print()\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucun exemple de d√©monstration trouv√©\")\n",
    "        print(\"   (GEPA peut optimiser uniquement les instructions)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Structure du module diff√©rente\")\n",
    "    print(\"   L'optimisation s'est concentr√©e sur d'autres aspects\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üí° Ce que GEPA a fait:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Analys√© les erreurs sur les donn√©es d'entra√Ænement\")\n",
    "print(\"‚úÖ G√©n√©r√© des variantes de prompts\")\n",
    "print(\"‚úÖ Utilis√© la r√©flexion LLM pour proposer des am√©liorations\")\n",
    "print(\"‚úÖ S√©lectionn√© la meilleure configuration via Pareto\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Tester le classifier optimis√©\n",
    "\n",
    "Maintenant que GEPA a optimis√© notre classifier, testons-le sur de nouveaux exemples pour voir la diff√©rence.\n",
    "\n",
    "### Comparaison c√¥te √† c√¥te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Test comparatif: Baseline vs GEPA optimis√©\\n\")\n",
    "\n",
    "# Exemples de test\n",
    "test_cases = [\n",
    "    {\n",
    "        'ticket': \"Mon ordinateur portable ne s'allume plus, j'ai une r√©union importante dans 1h\",\n",
    "        'expected_category': 'Hardware',\n",
    "        'expected_priority': 'Urgent'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Je voudrais acc√®s √† la base de donn√©es pour faire des analyses\",\n",
    "        'expected_category': 'Account',\n",
    "        'expected_priority': 'Medium'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Le WiFi est compl√®tement HS dans tout le b√¢timent!\",\n",
    "        'expected_category': 'Network',\n",
    "        'expected_priority': 'Critical'\n",
    "    },\n",
    "    {\n",
    "        'ticket': \"Mon logiciel de comptabilit√© plante quand j'exporte en PDF\",\n",
    "        'expected_category': 'Software',\n",
    "        'expected_priority': 'High'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Ticket':<50} | {'Attendu':<20} | {'Baseline':<20} | {'GEPA':<20}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "correct_baseline = 0\n",
    "correct_gepa = 0\n",
    "\n",
    "for test in test_cases:\n",
    "    ticket = test['ticket']\n",
    "    expected = f\"{test['expected_category']}/{test['expected_priority']}\"\n",
    "    \n",
    "    # Pr√©diction baseline\n",
    "    pred_baseline = baseline_classifier(ticket=ticket)\n",
    "    baseline_result = f\"{pred_baseline.category}/{pred_baseline.priority}\"\n",
    "    baseline_match = (pred_baseline.category == test['expected_category'] and \n",
    "                     pred_baseline.priority == test['expected_priority'])\n",
    "    \n",
    "    # Pr√©diction GEPA\n",
    "    pred_gepa = optimized_classifier(ticket=ticket)\n",
    "    gepa_result = f\"{pred_gepa.category}/{pred_gepa.priority}\"\n",
    "    gepa_match = (pred_gepa.category == test['expected_category'] and \n",
    "                 pred_gepa.priority == test['expected_priority'])\n",
    "    \n",
    "    if baseline_match:\n",
    "        correct_baseline += 1\n",
    "    if gepa_match:\n",
    "        correct_gepa += 1\n",
    "    \n",
    "    # Afficher avec indicateurs de succ√®s\n",
    "    baseline_icon = \"‚úÖ\" if baseline_match else \"‚ùå\"\n",
    "    gepa_icon = \"‚úÖ\" if gepa_match else \"‚ùå\"\n",
    "    \n",
    "    print(f\"{ticket[:48]:<50} | {expected:<20} | {baseline_icon} {baseline_result:<17} | {gepa_icon} {gepa_result:<17}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Pr√©cision sur les exemples de test:\")\n",
    "print(f\"  Baseline: {correct_baseline}/{len(test_cases)} ({correct_baseline/len(test_cases)*100:.0f}%)\")\n",
    "print(f\"  GEPA:     {correct_gepa}/{len(test_cases)} ({correct_gepa/len(test_cases)*100:.0f}%)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Conseils pour optimiser avec GEPA\n",
    "\n",
    "### 7.6.1 Qualit√© des donn√©es\n",
    "\n",
    "La qualit√© de l'optimisation GEPA d√©pend **fortement** de vos donn√©es :\n",
    "\n",
    "‚úÖ **Bonnes pratiques** :\n",
    "- Minimum 15-20 exemples d'entra√Ænement (id√©alement 30-50)\n",
    "- Exemples **diversifi√©s** couvrant tous les cas d'usage\n",
    "- Labels **corrects** et **coh√©rents**\n",
    "- Donn√©es de validation **s√©par√©es** du trainset\n",
    "\n",
    "‚ùå **√Ä √©viter** :\n",
    "- Trop peu d'exemples (<10)\n",
    "- Exemples r√©p√©titifs ou tr√®s similaires\n",
    "- Labels incoh√©rents ou ambigus\n",
    "- Utiliser les m√™mes donn√©es pour train et validation\n",
    "\n",
    "### 7.6.2 Choix de la m√©trique\n",
    "\n",
    "Votre **m√©trique d√©termine ce que GEPA optimise** :\n",
    "\n",
    "```python\n",
    "# M√©trique stricte (tout ou rien)\n",
    "def exact_match(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    return 1.0 if (prediction.category == example.category and \n",
    "                   prediction.priority == example.priority) else 0.0\n",
    "\n",
    "# M√©trique avec cr√©dit partiel (souvent meilleure pour GEPA)\n",
    "def partial_match(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    category_match = prediction.category == example.category\n",
    "    priority_match = prediction.priority == example.priority\n",
    "    \n",
    "    if category_match and priority_match:\n",
    "        return 1.0\n",
    "    elif category_match:\n",
    "        return 0.7  # Cat√©gorie plus importante\n",
    "    elif priority_match:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0\n",
    "```\n",
    "\n",
    "üí° **Conseil** : Les m√©triques avec cr√©dit partiel donnent g√©n√©ralement de meilleurs r√©sultats car elles fournissent plus de signal d'apprentissage √† GEPA.\n",
    "\n",
    "### 7.6.3 Choix du niveau d'optimisation\n",
    "\n",
    "| Situation | Niveau recommand√© | Raison |\n",
    "|-----------|------------------|---------|\n",
    "| Premi√®re exp√©rimentation | **light** | Test rapide du concept |\n",
    "| Prototype pour d√©mo | **light** | Balance vitesse/performance |\n",
    "| Application production (non-critique) | **medium** | Bon compromis |\n",
    "| Application production (critique) | **heavy** | Maximum de performance |\n",
    "| Recherche / benchmark | **heavy** | Explorer les limites |\n",
    "\n",
    "### 7.6.4 Configuration du mod√®le de r√©flexion\n",
    "\n",
    "Le **reflection_lm** influence la qualit√© des am√©liorations :\n",
    "\n",
    "‚úÖ **Bonnes pratiques** :\n",
    "- Utiliser un mod√®le avec bonne capacit√© de raisonnement\n",
    "- Temp√©rature √©lev√©e (0.8-1.2) pour la cr√©ativit√©\n",
    "- Max tokens √©lev√© (6000-10000) pour analyses d√©taill√©es\n",
    "- Peut √™tre le m√™me mod√®le que le mod√®le principal\n",
    "\n",
    "‚ùå **√Ä √©viter** :\n",
    "- Mod√®les trop petits (<7B param√®tres)\n",
    "- Temp√©rature trop basse (<0.5)\n",
    "- Max tokens trop faible (<4000)\n",
    "\n",
    "### 7.6.5 √âviter le surapprentissage\n",
    "\n",
    "GEPA peut **surapprendre** sur les donn√©es d'entra√Ænement :\n",
    "\n",
    "‚úÖ **Pr√©vention** :\n",
    "- Toujours avoir un **valset s√©par√©**\n",
    "- Valider sur de **nouvelles donn√©es** apr√®s optimisation\n",
    "- Comparer les scores train vs validation\n",
    "- Si grand √©cart : vos donn√©es ne sont pas assez diversifi√©es\n",
    "\n",
    "```python\n",
    "# Bon: √âvaluation sur donn√©es s√©par√©es\n",
    "score_train = evaluate_module(optimized, train_examples, metric)\n",
    "score_val = evaluate_module(optimized, val_examples, metric)\n",
    "\n",
    "if score_train - score_val > 0.2:\n",
    "    print(\"‚ö†Ô∏è Possible surapprentissage!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Troubleshooting et erreurs courantes\n",
    "\n",
    "### Erreur 1: `TypeError: GEPA.__init__() got an unexpected keyword argument`\n",
    "\n",
    "**Sympt√¥me** :\n",
    "```\n",
    "TypeError: GEPA.__init__() got an unexpected keyword argument 'breadth'\n",
    "```\n",
    "\n",
    "**Cause** : Utilisation de param√®tres de l'ancienne API GEPA (pre-3.0)\n",
    "\n",
    "**Solution** :\n",
    "```python\n",
    "# ‚ùå Ancienne API (ne fonctionne plus)\n",
    "optimizer = GEPA(\n",
    "    metric=my_metric,\n",
    "    breadth=10,\n",
    "    depth=3\n",
    ")\n",
    "\n",
    "# ‚úÖ Nouvelle API (DSPy 3.0+)\n",
    "optimizer = GEPA(\n",
    "    metric=my_metric,\n",
    "    auto='light',  # ou 'medium', 'heavy'\n",
    "    reflection_lm=reflection_lm\n",
    ")\n",
    "```\n",
    "\n",
    "### Erreur 2: `TypeError: metric() missing required positional argument`\n",
    "\n",
    "**Sympt√¥me** :\n",
    "```\n",
    "TypeError: metric() missing 2 required positional arguments: 'pred_name' and 'pred_trace'\n",
    "```\n",
    "\n",
    "**Cause** : M√©trique pas compatible avec l'API GEPA\n",
    "\n",
    "**Solution** : Ajouter les param√®tres optionnels √† votre m√©trique\n",
    "```python\n",
    "# ‚ùå Ancienne signature\n",
    "def my_metric(example, prediction):\n",
    "    return 1.0 if prediction.category == example.category else 0.0\n",
    "\n",
    "# ‚úÖ Nouvelle signature (compatible GEPA)\n",
    "def my_metric(example, prediction, trace=None, pred_name=None, pred_trace=None):\n",
    "    return 1.0 if prediction.category == example.category else 0.0\n",
    "```\n",
    "\n",
    "### Erreur 3: Ollama timeout ou erreur de connexion\n",
    "\n",
    "**Sympt√¥me** :\n",
    "```\n",
    "ConnectionError: Failed to connect to Ollama\n",
    "```\n",
    "\n",
    "**Causes possibles** :\n",
    "1. Ollama n'est pas d√©marr√©\n",
    "2. Mod√®le non t√©l√©charg√©\n",
    "3. M√©moire insuffisante\n",
    "\n",
    "**Solutions** :\n",
    "```bash\n",
    "# 1. V√©rifier qu'Ollama tourne\n",
    "ollama list\n",
    "\n",
    "# 2. D√©marrer Ollama si n√©cessaire\n",
    "ollama serve\n",
    "\n",
    "# 3. T√©l√©charger le mod√®le\n",
    "ollama pull llama3.1:8b\n",
    "\n",
    "# 4. V√©rifier la m√©moire disponible\n",
    "# GEPA + Ollama n√©cessite ~8-12 GB RAM\n",
    "```\n",
    "\n",
    "### Erreur 4: GEPA ne s'am√©liore pas\n",
    "\n",
    "**Sympt√¥me** : Score apr√®s optimisation ‚âà score avant\n",
    "\n",
    "**Causes possibles** :\n",
    "1. Donn√©es d'entra√Ænement insuffisantes ou de mauvaise qualit√©\n",
    "2. M√©trique mal d√©finie\n",
    "3. T√¢che trop difficile pour le mod√®le\n",
    "4. Module d√©j√† bien optimis√©\n",
    "\n",
    "**Solutions** :\n",
    "```python\n",
    "# V√©rifier la qualit√© des donn√©es\n",
    "print(f\"Nombre d'exemples train: {len(trainset)}\")\n",
    "print(f\"Nombre d'exemples val: {len(valset)}\")\n",
    "\n",
    "# V√©rifier la m√©trique\n",
    "for ex in trainset[:5]:\n",
    "    pred = baseline(ticket=ex['ticket'])\n",
    "    score = metric(ex, pred)\n",
    "    print(f\"Score: {score} | Pred: {pred.category}/{pred.priority} | Truth: {ex['category']}/{ex['priority']}\")\n",
    "\n",
    "# Essayer un niveau plus √©lev√©\n",
    "optimizer = GEPA(\n",
    "    metric=metric,\n",
    "    auto='heavy',  # au lieu de 'light'\n",
    "    reflection_lm=reflection_lm\n",
    ")\n",
    "```\n",
    "\n",
    "### Erreur 5: Out of Memory (OOM)\n",
    "\n",
    "**Sympt√¥me** : Ollama ou Python crash avec erreur de m√©moire\n",
    "\n",
    "**Solutions** :\n",
    "1. Utiliser un mod√®le plus petit : `mistral:7b` au lieu de `llama3.1:8b`\n",
    "2. R√©duire `max_tokens` du reflection_lm\n",
    "3. Utiliser `auto='light'` au lieu de `medium` ou `heavy`\n",
    "4. Fermer les autres applications\n",
    "5. Utiliser une API cloud au lieu d'Ollama local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 R√©sum√© de la Partie 7\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **GEPA : L'optimiseur le plus sophistiqu√©**\n",
    "   - Combine algorithmes g√©n√©tiques + r√©flexion LLM + Pareto\n",
    "   - Am√©lioration typique : 15-30%\n",
    "   - N√©cessite un mod√®le de r√©flexion (reflection_lm)\n",
    "\n",
    "2. **Configuration essentielle**\n",
    "   - `auto` : 'light', 'medium', ou 'heavy'\n",
    "   - `reflection_lm` : Mod√®le pour analyser les erreurs (temp=1.0, max_tokens=8000)\n",
    "   - `metric` : Doit accepter les param√®tres GEPA (trace, pred_name, pred_trace)\n",
    "\n",
    "3. **Niveaux d'optimisation**\n",
    "   - **light** : 5-10 min, ~200-400 appels LLM, 10-20% am√©lioration\n",
    "   - **medium** : 10-20 min, ~400-800 appels LLM, 15-25% am√©lioration\n",
    "   - **heavy** : 20-40 min, ~800-1600 appels LLM, 20-30% am√©lioration\n",
    "\n",
    "4. **Inspection des r√©sultats**\n",
    "   - Voir les prompts optimis√©s\n",
    "   - Comprendre les changements\n",
    "   - Valider les am√©liorations\n",
    "\n",
    "5. **Bonnes pratiques**\n",
    "   - Donn√©es : 15-20+ exemples diversifi√©s\n",
    "   - M√©trique : Cr√©dit partiel souvent meilleur\n",
    "   - Validation : Toujours sur donn√©es s√©par√©es\n",
    "   - Mod√®le : ‚â•7B param√®tres recommand√©\n",
    "\n",
    "6. **Troubleshooting**\n",
    "   - Erreurs d'API : V√©rifier la signature de la m√©trique\n",
    "   - Pas d'am√©lioration : V√©rifier qualit√© des donn√©es\n",
    "   - OOM : R√©duire niveau ou utiliser mod√®le plus petit\n",
    "\n",
    "### Points cl√©s √† retenir\n",
    "\n",
    "- ‚úÖ **GEPA est puissant** mais n√©cessite temps et ressources\n",
    "- ‚úÖ **Commencer avec 'light'** pour tester le concept\n",
    "- ‚úÖ **Qualit√© des donn√©es = qualit√© des r√©sultats**\n",
    "- ‚úÖ **Toujours valider** sur donn√©es s√©par√©es\n",
    "- ‚úÖ **Inspecter les prompts** pour comprendre les am√©liorations\n",
    "\n",
    "### Quand utiliser GEPA?\n",
    "\n",
    "| Situation | GEPA? | Alternative |\n",
    "|-----------|-------|-------------|\n",
    "| Prototypage rapide | ‚ùå Non | Module simple |\n",
    "| Tests initiaux | ‚ùå Non | BootstrapFewShot |\n",
    "| Application production (non-critique) | ‚ö†Ô∏è Peut-√™tre | MIPRO |\n",
    "| Application production (critique) | ‚úÖ Oui | GEPA medium/heavy |\n",
    "| Recherche / Maximum performance | ‚úÖ Oui | GEPA heavy |\n",
    "| Ressources limit√©es | ‚ùå Non | BootstrapFewShot |\n",
    "\n",
    "### Workflow recommand√©\n",
    "\n",
    "```python\n",
    "# Phase 1: Baseline\n",
    "classifier = SimpleTicketClassifier()\n",
    "score_baseline = evaluate(classifier, valset, metric)\n",
    "\n",
    "# Phase 2: Optimisation simple\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "optimizer_simple = BootstrapFewShot(metric=metric)\n",
    "classifier_simple = optimizer_simple.compile(classifier, trainset)\n",
    "score_simple = evaluate(classifier_simple, valset, metric)\n",
    "\n",
    "# Phase 3: GEPA (si am√©lioration justifie le temps)\n",
    "if score_simple < target_score:\n",
    "    from dspy.teleprompt import GEPA\n",
    "    optimizer_gepa = GEPA(metric=metric, auto='light', reflection_lm=reflection_lm)\n",
    "    classifier_gepa = optimizer_gepa.compile(classifier, trainset, valset)\n",
    "    score_gepa = evaluate(classifier_gepa, valset, metric)\n",
    "    \n",
    "    # Phase 4: GEPA heavy si n√©cessaire\n",
    "    if score_gepa < target_score:\n",
    "        optimizer_heavy = GEPA(metric=metric, auto='heavy', reflection_lm=reflection_lm)\n",
    "        classifier_final = optimizer_heavy.compile(classifier, trainset, valset)\n",
    "```\n",
    "\n",
    "### Ressources additionnelles\n",
    "\n",
    "- üìÑ [Paper GEPA (arXiv)](https://arxiv.org/abs/2507.19457)\n",
    "- üíª [GitHub GEPA](https://github.com/gepa-ai/gepa)\n",
    "- üìñ [Documentation DSPy](https://dspy-docs.vercel.app/)\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- **Partie 8** : Conclusion et mise en production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-gepa-demo (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
