{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration et pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurer le mod√®le de langage\n",
    "lm = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Configurer DSPy globalement\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es d'entra√Ænement\n",
    "trainset = [\n",
    "    {\"ticket\": \"Mon ordinateur ne d√©marre plus depuis ce matin. J'ai une pr√©sentation importante dans 2 heures.\", \"category\": \"Hardware\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je n'arrive pas √† me connecter √† l'imprimante du 3e √©tage. √áa peut attendre.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le VPN ne fonctionne plus. Impossible d'acc√©der aux fichiers du serveur.\", \"category\": \"Network\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"J'ai oubli√© mon mot de passe Outlook. Je peux utiliser le webmail.\", \"category\": \"Account\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le site web affiche une erreur 500. Les clients ne peuvent plus commander!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Ma souris sans fil ne r√©pond plus bien. Les piles sont faibles.\", \"category\": \"Peripherals\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le syst√®me de paie ne calcule pas les heures suppl√©mentaires. C'est la fin du mois.\", \"category\": \"Application\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"J'aimerais une mise √† jour de mon logiciel Adobe quand vous aurez le temps.\", \"category\": \"Software\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le serveur de base de donn√©es est tr√®s lent. Toute la production est impact√©e.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Je ne re√ßois plus les emails. J'attends des r√©ponses de fournisseurs.\", \"category\": \"Email\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon √©cran externe ne s'affiche plus. Je peux travailler sur le laptop.\", \"category\": \"Hardware\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le wifi de la salle A ne fonctionne pas. R√©union avec des externes dans 30 min.\", \"category\": \"Network\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"Je voudrais installer Slack pour mieux collaborer avec l'√©quipe.\", \"category\": \"Software\", \"priority\": \"Medium\"},\n",
    "    {\"ticket\": \"Le syst√®me de sauvegarde a √©chou√© cette nuit selon le rapport.\", \"category\": \"Infrastructure\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Mon clavier a une touche qui colle. C'est g√©rable mais ennuyeux.\", \"category\": \"Peripherals\", \"priority\": \"Low\"}\n",
    "]\n",
    "\n",
    "# Donn√©es de validation\n",
    "valset = [\n",
    "    {\"ticket\": \"Le serveur de fichiers est inaccessible. Personne ne peut travailler.\", \"category\": \"Infrastructure\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"J'ai besoin d'acc√®s au dossier comptabilit√© pour l'audit. C'est urgent.\", \"category\": \"Account\", \"priority\": \"Urgent\"},\n",
    "    {\"ticket\": \"L'√©cran de mon coll√®gue en vacances clignote. On peut attendre.\", \"category\": \"Hardware\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"Le CRM plante quand j'essaie d'exporter les contacts.\", \"category\": \"Application\", \"priority\": \"High\"},\n",
    "    {\"ticket\": \"Je voudrais changer ma photo de profil quand vous aurez un moment.\", \"category\": \"Account\", \"priority\": \"Low\"},\n",
    "    {\"ticket\": \"La vid√©oconf√©rence ne fonctionne pas. R√©union avec New York dans 10 minutes!\", \"category\": \"Application\", \"priority\": \"Critical\"},\n",
    "    {\"ticket\": \"Mon antivirus affiche un message d'expiration mais tout fonctionne.\", \"category\": \"Software\", \"priority\": \"Medium\"}\n",
    "]\n",
    "\n",
    "# Cat√©gories et priorit√©s possibles\n",
    "CATEGORIES = [\"Hardware\", \"Software\", \"Network\", \"Application\", \"Infrastructure\", \"Account\", \"Email\", \"Peripherals\"]\n",
    "PRIORITIES = [\"Low\", \"Medium\", \"High\", \"Urgent\", \"Critical\"]\n",
    "\n",
    "print(f\"üìä Donn√©es charg√©es : {len(trainset)} entra√Ænement, {len(valset)} validation\")\n",
    "print(f\"üì¶ {len(CATEGORIES)} cat√©gories, {len(PRIORITIES)} priorit√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 6: Multi-mod√®les et flexibilit√©\n",
    "\n",
    "## 5.1 Introduction: pourquoi utiliser plusieurs mod√®les?\n",
    "\n",
    "DSPy offre une **abstraction puissante** : votre code reste le m√™me quel que soit le mod√®le utilis√©. Vous pouvez :\n",
    "\n",
    "1. **Changer de fournisseur** facilement (Ollama ‚Üí OpenAI ‚Üí Anthropic)\n",
    "2. **Comparer les performances** de diff√©rents mod√®les\n",
    "3. **Cr√©er des architectures hybrides** (mod√®le rapide pour cat√©gorie, mod√®le pr√©cis pour priorit√©)\n",
    "4. **Optimiser co√ªt vs performance**\n",
    "\n",
    "### Avantages du multi-mod√®les\n",
    "\n",
    "- üîÑ **Flexibilit√©** : Pas de vendor lock-in\n",
    "- üí∞ **Optimisation des co√ªts** : Mod√®le gratuit (Ollama) pour dev, mod√®le payant pour prod\n",
    "- üéØ **Performance** : Choisir le meilleur mod√®le pour chaque t√¢che\n",
    "- üß™ **Exp√©rimentation** : Tester facilement diff√©rents mod√®les\n",
    "\n",
    "### Ce que nous allons voir\n",
    "\n",
    "1. Configurer diff√©rents fournisseurs (Ollama, OpenAI, Anthropic)\n",
    "2. Comparer les performances de diff√©rents mod√®les\n",
    "3. Cr√©er des architectures hybrides\n",
    "4. G√©rer les cl√©s API de mani√®re s√©curis√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Configuration de diff√©rents fournisseurs\n",
    "\n",
    "### 5.2.1 Ollama (local, gratuit)\n",
    "\n",
    "Ollama permet d'ex√©cuter des mod√®les **localement** sans API key ni co√ªts.\n",
    "\n",
    "**Mod√®les recommand√©s** :\n",
    "- `llama3.1:8b` - √âquilibr√©, bon pour la plupart des t√¢ches (4.7 GB)\n",
    "- `mistral:7b` - Rapide, bon pour les t√¢ches simples (4.1 GB)\n",
    "- `qwen2.5:7b` - Haute qualit√©, excellent pour les t√¢ches complexes (4.7 GB)\n",
    "- `gemma2:9b` - Alternative de Google, tr√®s performant (5.4 GB)\n",
    "\n",
    "**Configuration** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Ollama\n",
    "lm_ollama_llama = dspy.LM(\n",
    "    model='ollama_chat/llama3.1:8b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "lm_ollama_mistral = dspy.LM(\n",
    "    model='ollama_chat/mistral:7b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "lm_ollama_qwen = dspy.LM(\n",
    "    model='ollama_chat/qwen2.5:7b',\n",
    "    api_base='http://localhost:11434',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®les Ollama configur√©s:\")\n",
    "print(\"   - llama3.1:8b\")\n",
    "print(\"   - mistral:7b\")\n",
    "print(\"   - qwen2.5:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 OpenAI (API, payant)\n",
    "\n",
    "OpenAI propose des mod√®les tr√®s performants via API.\n",
    "\n",
    "**Mod√®les recommand√©s** :\n",
    "- `gpt-4o-mini` - Rapide et √©conomique, bon rapport qualit√©/prix\n",
    "- `gpt-4o` - Haute performance, multimodal\n",
    "- `gpt-4-turbo` - √âquilibr√© performance/co√ªt\n",
    "\n",
    "**Configuration** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuration OpenAI (n√©cessite OPENAI_API_KEY dans l'environnement)\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    lm_openai_mini = dspy.LM(\n",
    "        model='openai/gpt-4o-mini',\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    lm_openai_4o = dspy.LM(\n",
    "        model='openai/gpt-4o',\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Mod√®les OpenAI configur√©s:\")\n",
    "    print(\"   - gpt-4o-mini\")\n",
    "    print(\"   - gpt-4o\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OPENAI_API_KEY non d√©finie - mod√®les OpenAI non disponibles\")\n",
    "    print(\"   Pour utiliser OpenAI, d√©finissez la variable d'environnement:\")\n",
    "    print(\"   export OPENAI_API_KEY='votre-cl√©-api'\")\n",
    "    lm_openai_mini = None\n",
    "    lm_openai_4o = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Anthropic (API, payant)\n",
    "\n",
    "Anthropic propose les mod√®les Claude, connus pour leur qualit√© et leur s√©curit√©.\n",
    "\n",
    "**Mod√®les recommand√©s** :\n",
    "- `claude-3-5-haiku-20241022` - Rapide et √©conomique\n",
    "- `claude-3-5-sonnet-20241022` - √âquilibr√©, excellent pour la plupart des t√¢ches\n",
    "- `claude-3-opus-20240229` - Maximum de performance\n",
    "\n",
    "**Configuration** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Anthropic (n√©cessite ANTHROPIC_API_KEY dans l'environnement)\n",
    "if os.getenv('ANTHROPIC_API_KEY'):\n",
    "    lm_claude_haiku = dspy.LM(\n",
    "        model='anthropic/claude-3-5-haiku-20241022',\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    lm_claude_sonnet = dspy.LM(\n",
    "        model='anthropic/claude-3-5-sonnet-20241022',\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Mod√®les Anthropic configur√©s:\")\n",
    "    print(\"   - claude-3-5-haiku\")\n",
    "    print(\"   - claude-3-5-sonnet\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ANTHROPIC_API_KEY non d√©finie - mod√®les Anthropic non disponibles\")\n",
    "    print(\"   Pour utiliser Anthropic, d√©finissez la variable d'environnement:\")\n",
    "    print(\"   export ANTHROPIC_API_KEY='votre-cl√©-api'\")\n",
    "    lm_claude_haiku = None\n",
    "    lm_claude_sonnet = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Comparer les performances de diff√©rents mod√®les\n",
    "\n",
    "Maintenant que nous avons configur√© plusieurs mod√®les, comparons leurs performances sur notre t√¢che de classification de tickets IT.\n",
    "\n",
    "### Fonction de benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(lm, model_name, examples, metric):\n",
    "    \"\"\"\n",
    "    √âvalue un mod√®le sur un ensemble d'exemples\n",
    "    \n",
    "    Args:\n",
    "        lm: Le language model DSPy\n",
    "        model_name: Nom du mod√®le (pour affichage)\n",
    "        examples: Liste d'exemples de validation\n",
    "        metric: Fonction de m√©trique\n",
    "    \n",
    "    Returns:\n",
    "        dict avec score et temps d'ex√©cution\n",
    "    \"\"\"\n",
    "    # Configurer DSPy avec ce mod√®le\n",
    "    dspy.configure(lm=lm)\n",
    "    \n",
    "    # Cr√©er un classifier avec ce mod√®le\n",
    "    classifier = SimpleTicketClassifier()\n",
    "    \n",
    "    # Mesurer le temps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # √âvaluer\n",
    "    total_score = 0\n",
    "    for example in examples:\n",
    "        prediction = classifier(ticket=example['ticket'])\n",
    "        score = metric(example, prediction)\n",
    "        total_score += score\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculer les r√©sultats\n",
    "    avg_score = total_score / len(examples)\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'score': avg_score,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Fonction de benchmarking d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des mod√®les Ollama\n",
    "\n",
    "Comparons les 3 mod√®les Ollama locaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Comparaison des mod√®les Ollama...\")\n",
    "print(\"‚è∞ Cela va prendre quelques minutes\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Benchmarker llama3.1:8b\n",
    "print(\"1/3 √âvaluation de llama3.1:8b...\")\n",
    "result_llama = benchmark_model(lm_ollama_llama, 'llama3.1:8b', valset, exact_match_metric)\n",
    "results.append(result_llama)\n",
    "print(f\"   Score: {result_llama['score']:.2%} | Temps: {result_llama['time']:.1f}s\\n\")\n",
    "\n",
    "# Benchmarker mistral:7b\n",
    "print(\"2/3 √âvaluation de mistral:7b...\")\n",
    "result_mistral = benchmark_model(lm_ollama_mistral, 'mistral:7b', valset, exact_match_metric)\n",
    "results.append(result_mistral)\n",
    "print(f\"   Score: {result_mistral['score']:.2%} | Temps: {result_mistral['time']:.1f}s\\n\")\n",
    "\n",
    "# Benchmarker qwen2.5:7b\n",
    "print(\"3/3 √âvaluation de qwen2.5:7b...\")\n",
    "result_qwen = benchmark_model(lm_ollama_qwen, 'qwen2.5:7b', valset, exact_match_metric)\n",
    "results.append(result_qwen)\n",
    "print(f\"   Score: {result_qwen['score']:.2%} | Temps: {result_qwen['time']:.1f}s\\n\")\n",
    "\n",
    "# Afficher le r√©sum√©\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä R√âSUM√â DES PERFORMANCES\")\n",
    "print(\"=\" * 60)\n",
    "for r in sorted(results, key=lambda x: x['score'], reverse=True):\n",
    "    print(f\"{r['model']:20} | Score: {r['score']:6.2%} | Temps: {r['time']:5.1f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Architectures hybrides: utiliser diff√©rents mod√®les pour diff√©rentes t√¢ches\n",
    "\n",
    "Une **architecture hybride** utilise diff√©rents mod√®les pour diff√©rentes parties de votre pipeline. Par exemple :\n",
    "\n",
    "- Mod√®le **rapide et √©conomique** pour la cat√©gorisation\n",
    "- Mod√®le **pr√©cis mais co√ªteux** pour la priorisation\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- üí∞ **Optimisation des co√ªts** : Utiliser des mod√®les co√ªteux uniquement quand n√©cessaire\n",
    "- ‚ö° **Optimisation de la vitesse** : Mod√®les rapides pour les t√¢ches simples\n",
    "- üéØ **Optimisation de la qualit√©** : Meilleurs mod√®les pour les t√¢ches critiques\n",
    "\n",
    "### Exemple: pipeline hybride\n",
    "\n",
    "Cr√©ons un classifier qui utilise deux mod√®les diff√©rents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridTicketClassifier(dspy.Module):\n",
    "    \"\"\"\n",
    "    Classifier hybride utilisant 2 mod√®les diff√©rents:\n",
    "    - Mod√®le rapide pour la cat√©gorie\n",
    "    - Mod√®le pr√©cis pour la priorit√©\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fast_lm, accurate_lm):\n",
    "        super().__init__()\n",
    "        self.fast_lm = fast_lm\n",
    "        self.accurate_lm = accurate_lm\n",
    "        \n",
    "        # Signature pour la cat√©gorie\n",
    "        self.category_signature = CategoryClassifier\n",
    "        \n",
    "        # Signature pour la priorit√©\n",
    "        self.priority_signature = PriorityClassifier\n",
    "    \n",
    "    def forward(self, ticket):\n",
    "        # √âtape 1: Cat√©gorisation avec le mod√®le rapide\n",
    "        with dspy.settings.context(lm=self.fast_lm):\n",
    "            category_predictor = dspy.ChainOfThought(self.category_signature)\n",
    "            category_result = category_predictor(ticket=ticket)\n",
    "        \n",
    "        # √âtape 2: Priorisation avec le mod√®le pr√©cis\n",
    "        with dspy.settings.context(lm=self.accurate_lm):\n",
    "            priority_predictor = dspy.ChainOfThought(self.priority_signature)\n",
    "            priority_result = priority_predictor(\n",
    "                ticket=ticket,\n",
    "                category=category_result.category\n",
    "            )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            category=category_result.category,\n",
    "            priority=priority_result.priority\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Classe HybridTicketClassifier d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester le classifier hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÄ Test du classifier hybride\")\n",
    "print(\"   Mod√®le rapide (cat√©gorie): mistral:7b\")\n",
    "print(\"   Mod√®le pr√©cis (priorit√©): llama3.1:8b\\n\")\n",
    "\n",
    "# Cr√©er le classifier hybride\n",
    "hybrid_classifier = HybridTicketClassifier(\n",
    "    fast_lm=lm_ollama_mistral,      # Rapide pour la cat√©gorie\n",
    "    accurate_lm=lm_ollama_llama     # Pr√©cis pour la priorit√©\n",
    ")\n",
    "\n",
    "# Configurer DSPy (requis pour l'ex√©cution)\n",
    "dspy.configure(lm=lm_ollama_llama)\n",
    "\n",
    "# Tester sur quelques exemples\n",
    "print(\"üìù Exemples de pr√©dictions:\\n\")\n",
    "\n",
    "test_tickets = [\n",
    "    \"Mon ordinateur portable ne d√©marre plus, j'ai une pr√©sentation importante dans 2 heures\",\n",
    "    \"Je voudrais acc√®s au VPN pour le t√©l√©travail quand c'est possible\",\n",
    "    \"Toutes les imprimantes de l'√©tage sont hors ligne\"\n",
    "]\n",
    "\n",
    "for i, ticket in enumerate(test_tickets, 1):\n",
    "    result = hybrid_classifier(ticket=ticket)\n",
    "    print(f\"{i}. Ticket: {ticket[:60]}...\")\n",
    "    print(f\"   ‚Üí Cat√©gorie: {result.category} | Priorit√©: {result.priority}\\n\")\n",
    "\n",
    "# √âvaluer sur l'ensemble de validation\n",
    "print(\"üìä √âvaluation sur l'ensemble de validation:\")\n",
    "score_hybrid = evaluate_module(hybrid_classifier, val_examples, exact_match_metric)\n",
    "print(f\"   Score: {score_hybrid:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Guide de s√©lection de mod√®les\n",
    "\n",
    "### Crit√®res de s√©lection\n",
    "\n",
    "| Crit√®re | Ollama (local) | OpenAI | Anthropic |\n",
    "|---------|---------------|--------|-----------|\n",
    "| **Co√ªt** | Gratuit (mat√©riel local) | Payant √† l'usage | Payant √† l'usage |\n",
    "| **Vitesse** | D√©pend du mat√©riel | Rapide (API cloud) | Rapide (API cloud) |\n",
    "| **Confidentialit√©** | ‚úÖ 100% local | ‚ö†Ô∏è Donn√©es envoy√©es √† OpenAI | ‚ö†Ô∏è Donn√©es envoy√©es √† Anthropic |\n",
    "| **Qualit√©** | Bonne (7-8B params) | Excellente | Excellente |\n",
    "| **Disponibilit√©** | N√©cessite installation | API toujours disponible | API toujours disponible |\n",
    "| **Latence** | Faible (local) | Moyenne (r√©seau) | Moyenne (r√©seau) |\n",
    "\n",
    "### Recommandations par cas d'usage\n",
    "\n",
    "#### 1. D√©veloppement et prototypage\n",
    "**Choix recommand√©** : Ollama (llama3.1:8b ou qwen2.5:7b)\n",
    "- ‚úÖ Gratuit, it√©rations rapides\n",
    "- ‚úÖ Pas de limite de requ√™tes\n",
    "- ‚úÖ Confidentialit√© des donn√©es\n",
    "\n",
    "#### 2. Production √† faible volume (<1000 requ√™tes/jour)\n",
    "**Choix recommand√©** : OpenAI (gpt-4o-mini) ou Anthropic (claude-3-5-haiku)\n",
    "- ‚úÖ Co√ªts acceptables\n",
    "- ‚úÖ Haute disponibilit√©\n",
    "- ‚úÖ Excellente qualit√©\n",
    "\n",
    "#### 3. Production √† haut volume (>10000 requ√™tes/jour)\n",
    "**Choix recommand√©** : Architecture hybride\n",
    "- Ollama pour les t√¢ches simples (cat√©gorisation)\n",
    "- API payante pour les t√¢ches critiques (priorisation)\n",
    "- ‚úÖ Optimisation du rapport co√ªt/performance\n",
    "\n",
    "#### 4. Donn√©es sensibles (sant√©, finance, etc.)\n",
    "**Choix recommand√©** : Ollama uniquement\n",
    "- ‚úÖ Aucune donn√©e ne quitte votre infrastructure\n",
    "- ‚úÖ Conformit√© RGPD/HIPAA facilit√©e\n",
    "\n",
    "#### 5. Recherche et exp√©rimentation\n",
    "**Choix recommand√©** : Tous les mod√®les\n",
    "- Tester plusieurs mod√®les pour trouver le meilleur\n",
    "- Utiliser Ollama pour les it√©rations rapides\n",
    "- Valider avec des mod√®les API avant production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 R√©sum√© de la Partie 5\n",
    "\n",
    "### Ce que nous avons appris\n",
    "\n",
    "1. **Abstraction DSPy** : Un seul code fonctionne avec tous les fournisseurs LLM\n",
    "   \n",
    "2. **Configuration de fournisseurs** :\n",
    "   - **Ollama** : Local, gratuit, confidentialit√© maximale\n",
    "   - **OpenAI** : API cloud, haute qualit√©, payant\n",
    "   - **Anthropic** : API cloud, excellente qualit√©, payant\n",
    "\n",
    "3. **Comparaison de mod√®les** :\n",
    "   - Fonction de benchmarking pour comparer performances\n",
    "   - Mesure du score ET du temps d'ex√©cution\n",
    "   - Aide √† la d√©cision data-driven\n",
    "\n",
    "4. **Architectures hybrides** :\n",
    "   - Diff√©rents mod√®les pour diff√©rentes t√¢ches\n",
    "   - Optimisation co√ªt/performance/vitesse\n",
    "   - Utilisation de `dspy.settings.context(lm=...)`\n",
    "\n",
    "5. **Guide de s√©lection** :\n",
    "   - Crit√®res : co√ªt, vitesse, confidentialit√©, qualit√©\n",
    "   - Recommandations par cas d'usage\n",
    "   - Strat√©gies adapt√©es au contexte\n",
    "\n",
    "### Points cl√©s √† retenir\n",
    "\n",
    "- ‚úÖ **Flexibilit√©** : Changer de mod√®le ne n√©cessite que quelques lignes de code\n",
    "- ‚úÖ **Exp√©rimentation** : Tester plusieurs mod√®les est facile et recommand√©\n",
    "- ‚úÖ **Optimisation** : Architectures hybrides pour le meilleur rapport co√ªt/performance\n",
    "- ‚úÖ **Confidentialit√©** : Ollama pour les donn√©es sensibles\n",
    "- ‚úÖ **√âvolutivit√©** : Commencer avec Ollama, migrer vers API si n√©cessaire\n",
    "\n",
    "### Exemple de workflow recommand√©\n",
    "\n",
    "```python\n",
    "# 1. D√©veloppement avec Ollama (gratuit, rapide)\n",
    "lm_dev = dspy.LM('ollama_chat/llama3.1:8b', api_base='http://localhost:11434')\n",
    "dspy.configure(lm=lm_dev)\n",
    "\n",
    "# 2. Test avec plusieurs mod√®les\n",
    "models = [lm_ollama_llama, lm_ollama_qwen, lm_openai_mini]\n",
    "results = [benchmark_model(lm, name, valset, metric) for lm, name in models]\n",
    "\n",
    "# 3. Production avec le meilleur mod√®le ou architecture hybride\n",
    "lm_prod = best_model  # Ou HybridTicketClassifier(fast_lm, accurate_lm)\n",
    "```\n",
    "\n",
    "### Prochaines √©tapes\n",
    "\n",
    "- **Partie 6** : Patterns avanc√©s (optionnel - validation, retry, fallback)\n",
    "- **Partie 7** : GEPA en pratique (optimisation sophistiqu√©e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-gepa-demo (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
