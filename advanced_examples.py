"""
Exemples avanc√©s : Changer facilement entre diff√©rents LLMs
D√©montre la flexibilit√© de DSPy pour switcher entre fournisseurs
"""

import dspy
from data import trainset, valset

# =============================================================================
# Configuration 1: Ollama local (GRATUIT)
# =============================================================================

def configure_ollama(model_name="llama3.1:8b"):
    """Configure DSPy pour utiliser Ollama"""
    lm = dspy.LM(
        model=f'ollama_chat/{model_name}',
        api_base='http://localhost:11434',
        temperature=0.3
    )
    dspy.configure(lm=lm)
    print(f"‚úÖ Configur√© avec Ollama: {model_name}")

# =============================================================================
# Configuration 2: OpenAI (PAYANT)
# =============================================================================

def configure_openai(model_name="gpt-4o-mini"):
    """
    Configure DSPy pour utiliser OpenAI
    N√©cessite: export OPENAI_API_KEY="sk-..."
    """
    lm = dspy.LM(
        model=f'openai/{model_name}',
        temperature=0.3
    )
    dspy.configure(lm=lm)
    print(f"‚úÖ Configur√© avec OpenAI: {model_name}")

# =============================================================================
# Configuration 3: Anthropic Claude (PAYANT)
# =============================================================================

def configure_anthropic(model_name="claude-3-5-sonnet-20241022"):
    """
    Configure DSPy pour utiliser Anthropic Claude
    N√©cessite: export ANTHROPIC_API_KEY="sk-ant-..."
    """
    lm = dspy.LM(
        model=f'anthropic/{model_name}',
        temperature=0.3
    )
    dspy.configure(lm=lm)
    print(f"‚úÖ Configur√© avec Anthropic: {model_name}")

# =============================================================================
# Configuration 4: Plusieurs mod√®les dans le m√™me workflow
# =============================================================================

def configure_mixed_models():
    """
    Utiliser diff√©rents mod√®les pour diff√©rentes t√¢ches
    Ex: Un gros mod√®le pour la r√©flexion, un petit pour l'ex√©cution
    """
    # Mod√®le principal pour les t√¢ches courantes (rapide et gratuit)
    main_lm = dspy.LM(
        model='ollama_chat/llama3.1:8b',
        api_base='http://localhost:11434'
    )
    
    # Mod√®le avanc√© pour les t√¢ches complexes (optionnel, si disponible)
    # advanced_lm = dspy.LM(model='ollama_chat/llama3.1:70b', api_base='http://localhost:11434')
    
    dspy.configure(lm=main_lm)
    print("‚úÖ Configuration multi-mod√®les activ√©e")
    
    return main_lm  # , advanced_lm

# =============================================================================
# Exemple de workflow qui fonctionne avec N'IMPORTE QUEL LLM
# =============================================================================

class UniversalClassifier(dspy.Module):
    """
    Ce classifier fonctionne avec n'importe quel LLM configur√©!
    Changez juste la configuration avant de l'utiliser.
    """
    def __init__(self):
        super().__init__()
        self.classifier = dspy.ChainOfThought("ticket -> category, priority")
    
    def forward(self, ticket):
        return self.classifier(ticket=ticket)

# =============================================================================
# D√©monstration du changement de mod√®le √† la vol√©e
# =============================================================================

def demo_model_switching():
    """D√©montre comment changer de mod√®le facilement"""
    
    classifier = UniversalClassifier()
    test_ticket = "Mon ordinateur ne d√©marre plus et j'ai une r√©union importante dans 1 heure."
    
    print("\n" + "="*70)
    print("D√©monstration : M√™me code, diff√©rents mod√®les")
    print("="*70 + "\n")
    
    # Test avec Llama 3.1
    print("ü¶ô Test avec Llama 3.1 (Ollama local):")
    configure_ollama("llama3.1:8b")
    result = classifier(ticket=test_ticket)
    print(f"   Cat√©gorie: {result.category}, Priorit√©: {result.priority}\n")
    
    # Test avec Mistral (si disponible)
    print("üå™Ô∏è  Test avec Mistral (Ollama local):")
    try:
        configure_ollama("mistral:7b")
        result = classifier(ticket=test_ticket)
        print(f"   Cat√©gorie: {result.category}, Priorit√©: {result.priority}\n")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Mistral non disponible (faites: ollama pull mistral:7b)\n")
    
    # Test avec OpenAI (si cl√© API disponible)
    print("ü§ñ Test avec OpenAI GPT-4o-mini (si cl√© API configur√©e):")
    try:
        configure_openai("gpt-4o-mini")
        result = classifier(ticket=test_ticket)
        print(f"   Cat√©gorie: {result.category}, Priorit√©: {result.priority}\n")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  OpenAI non configur√© (export OPENAI_API_KEY n√©cessaire)\n")

# =============================================================================
# Liste des mod√®les Ollama populaires (gratuits)
# =============================================================================

RECOMMENDED_OLLAMA_MODELS = {
    "llama3.1:8b": "Meta Llama 3.1 - Excellent √©quilibre (4.7 GB)",
    "llama3.1:70b": "Meta Llama 3.1 - Tr√®s performant (40 GB, n√©cessite GPU)",
    "mistral:7b": "Mistral - Rapide et efficace (4.1 GB)",
    "qwen2.5:7b": "Qwen 2.5 - Tr√®s bon raisonnement (4.7 GB)",
    "deepseek-r1:7b": "DeepSeek R1 - Sp√©cialis√© raisonnement (4.7 GB)",
    "codellama:7b": "Code Llama - Pour code et technique (3.8 GB)",
    "phi3:3.8b": "Microsoft Phi-3 - Petit et rapide (2.3 GB)",
}

def print_available_models():
    """Affiche les mod√®les recommand√©s"""
    print("\nüì¶ Mod√®les Ollama recommand√©s (tous GRATUITS):\n")
    for model, description in RECOMMENDED_OLLAMA_MODELS.items():
        print(f"   ‚Ä¢ {model:20} - {description}")
    print("\nüí° Pour installer: ollama pull <model_name>")
    print("üí° Pour lister vos mod√®les: ollama list\n")

# =============================================================================
# Comparaison de performances entre mod√®les
# =============================================================================

def benchmark_models(models_to_test):
    """
    Compare la performance de diff√©rents mod√®les sur le m√™me dataset
    """
    from data import valset, CATEGORIES, PRIORITIES
    
    print("\n" + "="*70)
    print("üìä Benchmark : Comparaison de mod√®les")
    print("="*70 + "\n")
    
    results = {}
    
    for model_config in models_to_test:
        model_type = model_config['type']
        model_name = model_config['name']
        
        print(f"Testing {model_name}...")
        
        try:
            # Configure le mod√®le
            if model_type == 'ollama':
                configure_ollama(model_name)
            elif model_type == 'openai':
                configure_openai(model_name)
            elif model_type == 'anthropic':
                configure_anthropic(model_name)
            
            # Teste sur un √©chantillon
            classifier = UniversalClassifier()
            correct = 0
            
            for example in valset[:5]:  # Teste sur 5 exemples
                pred = classifier(ticket=example['ticket'])
                if (pred.category.lower() == example['category'].lower() and 
                    pred.priority.lower() == example['priority'].lower()):
                    correct += 1
            
            accuracy = (correct / 5) * 100
            results[model_name] = accuracy
            print(f"   ‚úÖ Pr√©cision: {accuracy}%\n")
            
        except Exception as e:
            print(f"   ‚ùå Erreur: {str(e)}\n")
            results[model_name] = None
    
    # R√©sum√©
    print("\nüìà R√©sum√© des performances:")
    for model, acc in results.items():
        if acc is not None:
            print(f"   {model:30} {acc}%")
        else:
            print(f"   {model:30} Non disponible")

# =============================================================================
# MAIN : Ex√©cuter les d√©monstrations
# =============================================================================

if __name__ == "__main__":
    print("\nüéØ Exemples avanc√©s : Configuration multi-LLM avec DSPy\n")
    
    # Afficher les mod√®les disponibles
    print_available_models()
    
    # D√©monstration du changement de mod√®le
    demo_model_switching()
    
    # Exemple de benchmark (d√©commentez pour l'ex√©cuter)
    """
    models = [
        {'type': 'ollama', 'name': 'llama3.1:8b'},
        {'type': 'ollama', 'name': 'mistral:7b'},
        # {'type': 'openai', 'name': 'gpt-4o-mini'},  # Si cl√© API configur√©e
    ]
    benchmark_models(models)
    """
    
    print("\n‚ú® D√©monstration termin√©e!")
    print("\nüí° Conseil: DSPy permet de changer de LLM en 1 ligne de code!")
    print("   D√©veloppez en local avec Ollama, d√©ployez avec OpenAI si besoin.")
